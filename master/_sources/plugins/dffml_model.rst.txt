:github_url: hide

.. _plugin_models:

Models
======

Models are implementations of :class:`dffml.model.model.Model`, they
abstract the usage of machine learning models.

If you want to get started creating your own model, check out the
:ref:`model_tutorial`.

.. _plugin_model_dffml:

dffml
-----

.. code-block:: console

    pip install dffml


.. _plugin_model_dffml_slr:

slr
~~~

*Official*

Logistic Regression training one variable to predict another.

The dataset used for training

.. literalinclude:: /../examples/model/slr/dataset.sh

Train the model

.. literalinclude:: /../examples/model/slr/train.sh

Assess the accuracy

.. literalinclude:: /../examples/model/slr/accuracy.sh

Output

.. code-block:: console

    1.0

Make a prediction

.. literalinclude:: /../examples/model/slr/predict.sh

Output

.. code-block:: console

    [
        {
            "extra": {},
            "features": {
                "ans": 0,
                "f1": 0.8
            },
            "last_updated": "2020-03-19T13:41:08Z",
            "prediction": {
                "ans": {
                    "confidence": 1.0,
                    "value": 1
                }
            },
            "key": "0"
        }
    ]

Example usage of Logistic Regression using Python

.. literalinclude:: /../examples/model/slr/slr.py

**Args**

- predict: Feature

  - Label or the value to be predicted

- features: List of features

  - Features to train on. For SLR only 1 allowed

- directory: Path

  - Directory where state should be saved

.. _plugin_model_dffml_model_tensorflow:

dffml_model_tensorflow
----------------------

.. code-block:: console

    pip install dffml-model-tensorflow


.. note::

    It's important to keep the hidden layer config and feature config the same
    across invocations of train, predict, and accuracy methods.

    Models are saved under the ``directory`` parameter in subdirectories named
    after the hash of their feature names and hidden layer config. Which means
    if any of those parameters change between invocations, it's being told to
    look for a different saved model.

.. _plugin_model_dffml_model_tensorflow_tfdnnc:

tfdnnc
~~~~~~

*Official*

Implemented using Tensorflow's DNNClassifier.

First we create the training and testing datasets

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/train_data.sh

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/accuracy.sh

Output

.. code-block::

    0.99996233782

Make a prediction

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "PetalLength": 4.2,
                "PetalWidth": 1.5,
                "SepalLength": 5.9,
                "SepalWidth": 3.0,
                "classification": 1
            },
            "last_updated": "2019-07-31T02:00:12Z",
            "prediction": {
                "classification":
                    {
                        "confidence": 0.9999997615814209,
                        "value": 1
                    }
            },
            "key": "0"
        },
    ]

Example usage of Tensorflow DNNClassifier model using python API

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/tfdnnc.py

**Args**

- predict: Feature

  - Feature name holding target values

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- steps: Integer

  - default: 3000
  - Number of steps to train the model

- epochs: Integer

  - default: 30
  - Number of iterations to pass over all records in a source

- hidden: List of integers

  - default: [12, 40, 15]
  - List length is the number of hidden layers in the network. Each entry in the list is the number of nodes in that hidden layer

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- batchsize: Integer

  - default: 20
  - Number records to pass through in an epoch

- shuffle: String

  - default: True
  - Randomise order of records in a batch

.. _plugin_model_dffml_model_tensorflow_tfdnnr:

tfdnnr
~~~~~~

*Official*

Implemented using Tensorflow's DNNEstimator.

Usage:

* predict: Name of the feature we are trying to predict or using for training.

Generating train and test data

* This creates files `train.csv` and `test.csv`,
  make sure to take a BACKUP of files with same name in the directory
  from where this command is run as it overwrites any existing files.

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/train_data.sh

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/accuracy.sh

Output

.. code-block::

    0.9468210011

Make a prediction

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "Feature1": 0.21,
                "Feature2": 0.18,
                "TARGET": 0.84
            },
            "last_updated": "2019-10-24T15:26:41Z",
            "prediction": {
                "TARGET" : {
                    "confidence": NaN,
                    "value": 1.1983429193496704
                }
            },
            "key": 0
        }
    ]

Example usage of Tensorflow DNNEstimator model using python API

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/tfdnnr.py

The ``NaN`` in ``confidence`` is the expected behaviour. (See TODO in
predict).

**Args**

- predict: Feature

  - Feature name holding target values

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- steps: Integer

  - default: 3000
  - Number of steps to train the model

- epochs: Integer

  - default: 30
  - Number of iterations to pass over all records in a source

- hidden: List of integers

  - default: [12, 40, 15]
  - List length is the number of hidden layers in the network. Each entry in the list is the number of nodes in that hidden layer

.. _plugin_model_dffml_model_tensorflow_hub:

dffml_model_tensorflow_hub
--------------------------

.. code-block:: console

    pip install dffml-model-tensorflow-hub


.. _plugin_model_dffml_model_tensorflow_hub_text_classifier:

text_classifier
~~~~~~~~~~~~~~~

*Official*

Implemented using Tensorflow hub pretrained models.


.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/train_data.sh

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/accuracy.sh

Output

.. code-block::

    0.5

Make a prediction

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "sentence": "I am not feeling good",
                "sentiment": 0
            },
            "key": "0",
            "last_updated": "2020-05-14T20:14:30Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.9999992847442627,
                    "value": 1
                }
            }
        },
        {
            "extra": {},
            "features": {
                "sentence": "Our trip was full of adventures",
                "sentiment": 1
            },
            "key": "1",
            "last_updated": "2020-05-14T20:14:30Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.9999088048934937,
                    "value": 1
                }
            }
        }
    ]



Example usage of Tensorflow_hub Text Classifier model using python API

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/textclassifier.py

**Args**

- predict: Feature

  - Feature name holding classification value

- classifications: List of strings

  - Options for value of classification

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- trainable: String

  - default: True
  - Tweak pretrained model by training again

- batch_size: Integer

  - default: 120
  - Batch size

- max_seq_length: Integer

  - default: 256
  - Length of sentence, used in preprocessing of input for bert embedding

- add_layers: String

  - default: False
  - Add layers on the top of pretrianed model/layer

- embedType: String

  - default: None
  - Type of pretrained embedding model, required to be set to `bert` to use bert pretrained embedding

- layers: List of strings

  - default: None
  - Extra layers to be added on top of pretrained model

- model_path: String

  - default: https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1
  - Pretrained model path/url

- optimizer: String

  - default: adam
  - Optimizer used by model

- metrics: String

  - default: accuracy
  - Metric used to evaluate model

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- epochs: Integer

  - default: 10
  - Number of iterations to pass over all records in a source

.. _plugin_model_dffml_model_transformers:

dffml_model_transformers
------------------------

.. code-block:: console

    pip install dffml-model-transformers


.. _plugin_model_dffml_model_transformers_hfclassifier:

hfclassifier
~~~~~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Tensorflow based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets

.. literalinclude:: /../model/transformers/examples/classification/train_data.sh

.. literalinclude:: /../model/transformers/examples/classification/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/classification/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/classification/accuracy.sh

Output

.. code-block::

    0.6666666666666666


Make a prediction

.. literalinclude:: /../model/transformers/examples/classification/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "sentence": "Cats are stupid",
                "sentiment": 0
            },
            "key": "0",
            "last_updated": "2020-06-12T08:31:45Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.153812974691391,
                    "value": 1
                }
            }
        },
        {
            "extra": {},
            "features": {
                "sentence": "My office work is awesome",
                "sentiment": 1
            },
            "key": "1",
            "last_updated": "2020-06-12T08:31:45Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.13032104074954987,
                    "value": 1
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

**Args**

- features: List of features

  - Feature to train on

- predict: Feature

  - Feature holding target values

- label_list: List of strings

  - List of target labels

- cache_dir: String

  - Directory to store the pre-trained models downloaded from s3

- model_name_or_path: String

  - Path to pretrained model or model identifier from huggingface.co/models

- output_dir: String

  - The output directory where the model predictions and checkpoints will be written.

- logging_dir: String

  - Tensorboard log dir.

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- max_seq_length: Integer

  - default: 128
  - The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.

- overwrite_cache: String

  - default: False
  - Overwrite the cached training and evaluation sets

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- use_fast: String

  - default: False
  - Set this flag to use fast tokenization.

- doc_stride: Integer

  - default: 128
  - When splitting up a long document into chunks, how much stride to take between chunks.

- optimizer_name: String

  - default: adam
  - Name of a Tensorflow optimizer among "adadelta, adagrad, adam, adamax, ftrl, nadam, rmsprop, sgd, adamw"

- loss_name: String

  - default: SparseCategoricalCrossentropy
  - Name of a Tensorflow loss. For the list see: https://www.tensorflow.org/api_docs/python/tf/keras/losses

- gpus: String

  - default: 0
  - List of gpu devices. If only one, switch to single gpu strategy, if None takes all availabel gpus

- no_cuda: String

  - default: False
  - Do not use CUDA even when it is available

- end_lr: float

  - default: 0
  - End learning rate for optimizer

- debug: String

  - default: False
  - Activate the trace to record computation graphs and profiling information

- overwrite_output_dir: String

  - default: False
  - Overwrite the content of the output directory.Use this to continue training if output_dir points to a checkpoint directory.

- evaluate_during_training: String

  - default: False
  - Run evaluation during training at each logging step.

- per_device_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/TPU core/CPU for training.

- per_device_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/TPU core/CPU for evaluation.

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward/update pass.

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam.

- weight_decay: float

  - default: 0.0
  - Weight decay if we apply some.

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer.

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- num_train_epochs: float

  - default: 1
  - Total number of training epochs to perform.

- max_steps: Integer

  - default: -1
  - If > 0: set total number of training steps to perform. Override num_train_epochs.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- logging_first_step: String

  - default: False
  - Log and eval the first global_step

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_steps: Integer

  - default: 500
  - Save checkpoint every X updates steps.

- save_total_limit: Integer

  - default: None
  - Limit the total amount of checkpoints.Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints

- seed: Integer

  - default: 42
  - random seed for initialization

- fp16: String

  - default: False
  - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].See details at https://nvidia.github.io/apex/amp.html

- local_rank: Integer

  - default: -1
  - For distributed training: local_rank

.. _plugin_model_dffml_model_transformers_ner_tagger:

ner_tagger
~~~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Tensorflow based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets

.. literalinclude:: /../model/transformers/examples/ner/train_data.sh

.. literalinclude:: /../model/transformers/examples/ner/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/ner/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/ner/accuracy.sh

Output

.. code-block::

    0.888888888888889

Make a prediction

.. literalinclude:: /../model/transformers/examples/ner/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "SentenceId": 1,
                "Words": "DFFML models can do NER"
            },
            "key": "0",
            "last_updated": "2020-06-11T17:40:54Z",
            "prediction": {
                "Tag": {
                    "confidence": NaN,
                    "value": [
                        {
                            "DFFML": "B-MISC"
                        },
                        {
                            "models": "I-MISC"
                        },
                        {
                            "can": "O"
                        },
                        {
                            "do": "B-MISC"
                        },
                        {
                            "NER": "B-MISC"
                        }
                    ]
                }
            }
        },
        {
            "extra": {},
            "features": {
                "SentenceId": 2,
                "Words": "DFFML models can do regression"
            },
            "key": "1",
            "last_updated": "2020-06-11T17:40:57Z",
            "prediction": {
                "Tag": {
                    "confidence": NaN,
                    "value": [
                        {
                            "DFFML": "B-MISC"
                        },
                        {
                            "models": "I-MISC"
                        },
                        {
                            "can": "O"
                        },
                        {
                            "do": "B-MISC"
                        },
                        {
                            "regression": "I-MISC"
                        }
                    ]
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

Example usage of NER model using python API

.. literalinclude:: /../model/transformers/examples/ner/ner_model.py

**Args**

- sid: Feature

  - Unique Id to identify words of each sentence (Sentence ID)

- words: Feature

  - Tokens to train NER model

- predict: Feature

  - NER Tags (B-MISC, I-PER, O etc.) for tokens

- model_name_or_path: String

  - Path to pre-trained model or shortcut name listed on https://huggingface.co/models

- output_dir: String

  - The output directory where the model checkpoints will be written

- cache_dir: String

  - Directory to store the pre-trained models downloaded from s3

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- overwrite_output_dir: String

  - default: False
  - Overwrite the content of the output directory.Use this to continue training if output_dir points to a checkpoint directory.

- max_seq_length: Integer

  - default: 128
  - The maximum total input sentence length after tokenization.Sequences longer than this will be truncated, sequences shorter will be padded

- max_steps: Integer

  - default: 0
  - If greater than zero then sets total number of training steps to perform. Overrides `epochs`

- use_fp16: String

  - default: False
  - Whether to use 16-bit (mixed) precision instead of 32-bit

- use_fast: String

  - default: False
  - Set this flag to use fast tokenization.

- ner_tags: List of strings

  - default: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']
  - List of all distinct NER Tags

- do_lower_case: String

  - default: False
  - Set this flag if using uncased model.

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward pass.

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam

- weight_decay: float

  - default: 0.0
  - Weight decay

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- epochs: Integer

  - default: 1
  - Total number of training epochs to perform.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- save_steps: Integer

  - default: 10
  - Save checkpoint every X update steps.

- seed: Integer

  - default: 2020
  - Random seed for initialization

- gpus: String

  - default: 0
  - List of gpu devices. If only one, switch to single gpu strategy, if None takes all availabel gpus

- tpu: String

  - default: None
  - The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url

- num_tpu_cores: Integer

  - default: 8
  - Total number of TPU cores to use.

- per_device_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU/TPU for training

- per_device_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU/TPU for assessing accuracy

- no_cuda: String

  - default: False
  - Avoid using CUDA when available

- optimizer_name: String

  - default: adam
  - Name of a Tensorflow optimizer among "adadelta, adagrad, adam, adamax, ftrl, nadam, rmsprop, sgd, adamw"

- loss_name: String

  - default: SparseCategoricalCrossentropy
  - Name of a Tensorflow loss. For the list see: https://www.tensorflow.org/api_docs/python/tf/keras/losses

- overwrite_cache: String

  - default: False
  - Overwrite the cached training and evaluation sets

- logging_dir: String

  - default: ~/.cache/dffml/transformers/log
  - Tensorboard log dir.

- logging_first_step: String

  - default: False
  - Log and eval the first global_step

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_total_limit: Integer

  - default: None
  - Limit the total amount of checkpoints.Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].See details at https://nvidia.github.io/apex/amp.html

- local_rank: Integer

  - default: -1
  - For distributed training: local_rank

- end_lr: float

  - default: 0
  - End learning rate for optimizer

- debug: String

  - default: False
  - Activate the trace to record computation graphs and profiling information

- num_train_epochs: float

  - default: 1
  - Total number of training epochs to perform.

- evaluate_during_training: String

  - default: False
  - Run evaluation during training at each logging step.

.. _plugin_model_dffml_model_transformers_qa_model:

qa_model
~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Pytorch based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets by taking few data points from `SQuAD1.1 dataset <https://rajpurkar.github.io/SQuAD-explorer/>`_ `LICENSE <https://creativecommons.org/licenses/by-sa/4.0/legalcode>`_

.. literalinclude:: /../model/transformers/examples/qa/train_data.sh

.. literalinclude:: /../model/transformers/examples/qa/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/qa/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/qa/accuracy.sh

Output

.. code-block::

    0.0

Make a prediction

.. literalinclude:: /../model/transformers/examples/qa/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "answer_text": null,
                "answers": [
                    {
                        "answer_start": 0,
                        "text": " "
                    }
                ],
                "context": "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
                "is_impossible": false,
                "question": "What sits on top of the Main Building at Notre Dame?",
                "start_pos_char": null,
                "title": "University_of_Notre_Dame"
            },
            "key": "5733be284776f4190066117e",
            "last_updated": "2020-06-23T17:54:03Z",
            "prediction": {
                "Answer": {
                    "confidence": NaN,
                    "value": {
                        "5733be284776f4190066117e": "a copper statue of Christ"
                    }
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

In the above train, accuracy and predict commands, :ref:`plugin_source_dffml_op` source is used to
read and parse data from json file before feeding it to the model. The function used by opsource to parse json data
is:

.. literalinclude:: /../model/transformers/dffml_model_transformers/qa/utils.py

The location of the function is passed using: 

.. code-block:: console

        -source-opimp dffml_model_transformers.qa.utils:parser

And the arguments to `parser` are passed by:

.. code-block:: console

        -source-args train.json True

where `train.json` is the name of file containing training data and the bool `True`
is value of the flag `is_training`.

**Args**

- model_type: String

  - Model type in the list: distilbert, albert, longformer, roberta, bert, xlnet, flaubert, xlm

- model_name_or_path: String

  - Path to pretrained model or model identifier from huggingface.co/models

- output_dir: String

  - The output directory where the model checkpoints and predictions will be written.

- cache_dir: String

  - Where do you want to store the pre-trained models downloaded from s3

- log_dir: String

  - Directory used by SummaryWriter for logging

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- from_tf: String

  - default: False
  - Whether to load model from tensorflow checkpoint or .h5 file

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- null_score_diff_threshold: String

  - default: 0.0
  - If null_score - best_non_null is greater than the threshold predict null.

- max_seq_length: Integer

  - default: 384
  - The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.

- doc_stride: Integer

  - default: 128
  - When splitting up a long document into chunks, how much stride to take between chunks.

- max_query_length: Integer

  - default: 64
  - The maximum number of tokens for the question. Questions longer than this will be truncated to this length

- do_lower_case: String

  - default: False
  - Set this flag while using uncased model

- per_gpu_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU for training

- per_gpu_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU for evaluation

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward/update pass

- weight_decay: float

  - default: 0.0
  - Weight decay if we apply some.

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- num_train_epochs: float

  - default: 1.0
  - Total number of training epoches to perform

- max_steps: Integer

  - default: -1
  - If > 0: set total number of training steps to perform. Override num_train_epochs.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- n_best_size: Integer

  - default: 20
  - The total number of n-best predictions to generate

- max_answer_length: Integer

  - default: 30
  - The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.

- lang_id: Integer

  - default: 0
  - language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_steps: Integer

  - default: 500
  - Save checkpoint every X update steps

- no_cuda: String

  - default: False
  - Whether not to use CUDA when available

- overwrite_output_dir: String

  - default: False
  - Overwrite the content of the output directory

- seed: Integer

  - default: 2020
  - random seed for initialization

- local_rank: Integer

  - default: -1
  - local_rank for distributed training on gpus

- fp16: Integer

  - default: False
  - Whether to use 16-bit (mixed) precision (through NVIDIA apex) insted of 32-bit

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html

- threads: Integer

  - default: 1
  - Multiple threads for converting example to features

.. _plugin_model_dffml_model_scratch:

dffml_model_scratch
-------------------

.. code-block:: console

    pip install dffml-model-scratch


.. _plugin_model_dffml_model_scratch_scratchlgrsag:

scratchlgrsag
~~~~~~~~~~~~~

*Official*

Logistic Regression using stochastic average gradient descent optimizer

The dataset used for training

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/dataset.sh

Train the model

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/train.sh

Assess the accuracy

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/accuracy.sh

Output

.. code-block:: console

    1.0

Make a prediction

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/predict.sh

Output

.. code-block:: console

    [
        {
            "extra": {},
            "features": {
                "ans": 0,
                "f1": 0.8
            },
            "last_updated": "2020-03-19T13:41:08Z",
            "prediction": {
                "ans": {
                    "confidence": 1.0,
                    "value": 1
                }
            },
            "key": "0"
        }
    ]

Example usage of Logistic Regression using Python

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/scratchlgrsag.py

**Args**

- predict: Feature

  - Label or the value to be predicted

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

.. _plugin_model_dffml_model_scikit:

dffml_model_scikit
------------------

.. code-block:: console

    pip install dffml-model-scikit


Machine Learning models implemented with `scikit-learn <https://scikit-learn.org/stable/>`_.
Models are saved under the directory in subdirectories named after the hash of
their feature names.

**General Usage:**

Training:

.. code-block:: console

    $ dffml train \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -model-SCIKIT_PARAMETER_NAME SCIKIT_PARAMETER_VALUE \
        -sources f=TRAINING_DATA_SOURCE_TYPE \
        -source-filename TRAINING_DATA_FILE_NAME \
        -log debug

Testing and Accuracy:

.. code-block:: console

    $ dffml accuracy \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -sources f=TESTING_DATA_SOURCE_TYPE \
        -source-filename TESTING_DATA_FILE_NAME \
        -log debug

Predicting with trained model:

.. code-block:: console

    $ dffml predict all \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -sources f=PREDICT_DATA_SOURCE_TYPE \
        -source-filename PREDICT_DATA_FILE_NAME \
        -log debug


**Models Available:**

+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Type           | Model                         | Entrypoint     | Parameters                                                                                                                                                                                    |
+================+===============================+================+===============================================================================================================================================================================================+
| Regression     | LinearRegression              | scikitlr       | `scikitlr <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression/>`_                                             |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ElasticNet                    | scikiteln      | `scikiteln <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet/>`_                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RandomForestRegressor         | scikitrfr      | `scikitrfr <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html>`_                                                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BayesianRidge                 | scikitbyr      | `scikitbyr <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Lasso                         | scikitlas      | `scikitlas <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso/>`_                                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ARDRegression                 | scikitard      | `scikitard <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RANSACRegressor               | scikitrsc      | `scikitrsc <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor/>`_                                              |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | DecisionTreeRegressor         | scikitdtr      | `scikitdtr <https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianProcessRegressor      | scikitgpr      | `scikitgpr <https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor/>`_                    |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | OrthogonalMatchingPursuit     | scikitomp      | `scikitomp <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit/>`_                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Lars                          | scikitlars     | `scikitlars <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars/>`_                                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Ridge                         | scikitridge    | `scikitridge <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge/>`_                                                                |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Classification | KNeighborsClassifier          | scikitknn      | `scikitknn <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier/>`_                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AdaBoostClassifier            | scikitadaboost | `scikitadaboost <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier/>`_                                           |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianProcessClassifier     | scikitgpc      | `scikitgpc <https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier/>`_                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | DecisionTreeClassifier        | scikitdtc      | `scikitdtc <https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier/>`_                                                |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RandomForestClassifier        | scikitrfc      | `scikitrfc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier/>`_                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | QuadraticDiscriminantAnalysis | scikitqda      | `scikitqda <https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis/>`_|
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MLPClassifier                 | scikitmlp      | `scikitmlp <https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier/>`_                                              |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianNB                    | scikitgnb      | `scikitgnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB/>`_                                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | SVC                           | scikitsvc      | `scikitsvc <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC/>`_                                                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | LogisticRegression            | scikitlor      | `scikitlor <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression/>`_                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GradientBoostingClassifier    | scikitgbc      | `scikitgbc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier/>`_                                |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BernoulliNB                   | scikitbnb      | `scikitbnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB/>`_                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ExtraTreesClassifier          | scikitetc      | `scikitetc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier/>`_                                            |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BaggingClassifier             | scikitbgc      | `scikitbgc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | LinearDiscriminantAnalysis    | scikitlda      | `scikitlda <https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis/>`_      |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MultinomialNB                 | scikitmnb      | `scikitmnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB/>`_                                                    |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Clustering     | KMeans                        | scikitkmeans   | `scikitkmeans <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans/>`_                                                                       |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Birch                         | scikitbirch    | `scikitbirch <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch/>`_                                                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MiniBatchKMeans               | scikitmbkmeans | `scikitmbkmeans <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans/>`_                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AffinityPropagation           | scikitap       | `scikitap <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation/>`_                                                 |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MeanShift                     | scikitms       | `scikitms <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift/>`_                                                                     |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | SpectralClustering            | scikitsc       | `scikitsc <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering/>`_                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AgglomerativeClustering       | scikitac       | `scikitac <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering/>`_                                         |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | OPTICS                        | scikitoptics   | `scikitoptics <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html#sklearn.cluster.OPTICS/>`_                                                                       |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


**Usage Example:**

Example below uses LinearRegression Model using the command line.

Let us take a simple example:

+----------------------+------------+--------------+--------+
| Years of Experience  |  Expertise | Trust Factor | Salary |
+======================+============+==============+========+
|          0           |     01     |      0.2     |   10   |
+----------------------+------------+--------------+--------+
|          1           |     03     |      0.4     |   20   |
+----------------------+------------+--------------+--------+
|          2           |     05     |      0.6     |   30   |
+----------------------+------------+--------------+--------+
|          3           |     07     |      0.8     |   40   |
+----------------------+------------+--------------+--------+
|          4           |     09     |      1.0     |   50   |
+----------------------+------------+--------------+--------+
|          5           |     11     |      1.2     |   60   |
+----------------------+------------+--------------+--------+

First we create the files

.. literalinclude:: /../model/scikit/examples/lr/train_data.sh

.. literalinclude:: /../model/scikit/examples/lr/test_data.sh

Train the model

.. literalinclude:: /../model/scikit/examples/lr/train.sh

Assess accuracy

.. literalinclude:: /../model/scikit/examples/lr/accuracy.sh

Output:

.. code-block::

    1.0

Make a prediction

.. literalinclude:: /../model/scikit/examples/lr/predict.sh

Output:

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "Expertise": 13,
                "Trust": 0.7,
                "Years": 6
            },
            "key": "0",
            "last_updated": "2020-03-01T22:26:46Z",
            "prediction": {
                "Salary": {
                    "confidence": 1.0,
                    "value": 70.0
                }
            }
        }
    ]


Example usage of Linear Regression Model using python API:

.. literalinclude:: /../model/scikit/examples/lr/lr.py

Example below uses KMeans Clustering Model on a small randomly generated dataset.

.. code-block:: console

    $ cat > train.csv << EOF
   Col1,          Col2,        Col3,         Col4
   5.05776417,   8.55128116,   6.15193196,  -8.67349666
   3.48864265,  -7.25952218,  -4.89216256,   4.69308946
   -8.16207603,  5.16792984,  -2.66971993,   0.2401882
   6.09809669,   8.36434181,   6.70940915,  -7.91491768
   -9.39122566,  5.39133807,  -2.29760281,  -1.69672981
   0.48311336,   8.19998973,   7.78641979,   7.8843821
   2.22409135,  -7.73598586,  -4.02660224,   2.82101794
   2.8137247 ,   8.36064298,   7.66196849,   3.12704676
   EOF
    $ cat > test.csv << EOF
   Col1,             Col2,          Col3,         Col4,    cluster
   -10.16770144,   2.73057215,  -1.49351481,   2.43005691,    6
   3.59705381,  -4.76520663,  -3.34916068,   5.72391486,     1
   4.01612313,  -4.641852  ,  -4.77333308,   5.87551683,     0
   EOF
    $ dffml train \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename train.csv \
        -source-readonly \
        -log debug
    $ dffml accuracy \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1\
        -model-tcluster cluster:int:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename test.csv \
        -source-readonly \
        -log debug
    0.6365141682948129
    $ echo -e 'Col1,Col2,Col3,Col4\n6.09809669,8.36434181,6.70940915,-7.91491768\n' | \
      dffml predict all \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename /dev/stdin \
        -source-readonly \
        -log debug
    [
        {
            "extra": {},
            "features": {
                "Col1": 6.09809669,
                "Col2": 8.36434181,
                "Col3": 6.70940915,
                "Col4": -7.91491768
            },
            "last_updated": "2020-01-12T22:51:15Z",
            "prediction": {
                "confidence": 0.6365141682948129,
                "value": 2
            },
            "key": "0"
        }
    ]

Example usage of KMeans Clustering Model using python API:

.. code-block:: python

    from dffml import CSVSource, Features, Feature
    from dffml.noasync import train, accuracy, predict
    from dffml_model_scikit import KMeansModel

    model = KMeansModel(
        features=Features(
            Feature("Col1", float, 1),
            Feature("Col2", float, 1),
            Feature("Col3", float, 1),
            Feature("Col4", float, 1),
        ),
        tcluster=Feature("cluster", int, 1),
        directory="tempdir",
    )

    # Train the model
    train(model, "train.csv")

    # Assess accuracy (alternate way of specifying data source)
    print("Accuracy:", accuracy(model, CSVSource(filename="test.csv")))

    # Make prediction
    for i, features, prediction in predict(
        model,
        {"Col1": 6.09809669, "Col2": 8.36434181, "Col3": 6.70940915, "Col4": -7.91491768},
    ):
        features["cluster"] = prediction["cluster"]["value"]
        print(features)

**NOTE**: `Transductive <https://scikit-learn.org/stable/glossary.html#term-transductive/>`_ Clusterers(scikitsc, scikitac, scikitoptics) cannot handle unseen data.
Ensure that `predict` and `accuracy` for these algorithms uses training data.

**Args**

- predict: Feature

  - Label or the value to be predicted
  - Only used by classification and regression models

- tcluster: Feature

  - True cluster, only used by clustering models
  - Passed with `accuracy` to return `mutual_info_score`
  - If not passed `accuracy` returns `silhouette_score`

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved



.. _plugin_model_dffml_model_vowpalWabbit:

dffml_model_vowpalWabbit
------------------------

.. code-block:: console

    pip install dffml-model-vowpalWabbit


.. _plugin_model_dffml_model_vowpalWabbit_vwmodel:

vwmodel
~~~~~~~

*Official*

Implemented using Vowpal Wabbit.

First we create the training and testing datasets

.. literalinclude:: /../model/vowpalWabbit/examples/train_data.sh

.. literalinclude:: /../model/vowpalWabbit/examples/test_data.sh

Train the model

.. literalinclude:: /../model/vowpalWabbit/examples/train.sh

Assess the accuracy

.. literalinclude:: /../model/vowpalWabbit/examples/accuracy.sh

Output

.. code-block::

    0.38683876649129145


Make a prediction

.. literalinclude:: /../model/vowpalWabbit/examples/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "A": "| price:.46 sqft:.4 age:.10 1924"
            },
            "key": "0",
            "last_updated": "2020-05-29T16:36:57Z",
            "prediction": {
                "B": {
                    "confidence": 0.38683876649129145,
                    "value": 0.0
                }
            }
        }
    ]

**Args**

- features: List of features

- predict: Feature

  - Feature to predict

- directory: Path

  - Directory where state should be saved

- class_cost: List of features

  - default: None
  - Features with name `Cost_{class}` contaning cost of `class` for each input example, used when `csoaa` is used

- task: String

  - default: regression
  - Task to perform, possible values are `classification`, `regression`

- use_binary_label: String

  - default: False
  - Convert target labels to -1 and 1 for binary classification

- vwcmd: List of strings

  - default: []
  - Command Line Arguements as per vowpal wabbit convention

- namespace: List of strings

  - default: []
  - Namespace for input features. Should be in format {namespace}_{feature name}

- importance: Feature

  - default: None
  - Feature containing `importance` of each example, used in conversion of input data to vowpal wabbit input format

- base: Feature

  - default: None
  - Feature containing `base` for each example, used for residual regression

- tag: Feature

  - default: None
  - Feature to be used as `tag` in conversion of data to vowpal wabbit input format

- noconvert: String

  - default: False
  - Do not convert record features to vowpal wabbit input format