:github_url: hide

.. _plugin_models:

Models
======

Models are implementations of :class:`dffml.model.model.Model`, they
abstract the usage of machine learning models.

If you want to get started creating your own model, check out the
:ref:`model_tutorial`.

.. _plugin_model_dffml:

dffml
+++++

.. code-block:: console

    pip install dffml


.. _plugin_model_dffml_slr:

slr
~~~

*Official*

Logistic Regression training one variable to predict another.

The dataset used for training

**dataset.csv**

.. code-block::
    :test:
    :filepath: dataset.csv

    f1,ans
    0.1,0
    0.7,1
    0.6,1
    0.2,0
    0.8,1

Train the model

.. code-block:: console
    :test:

    $ dffml train \
        -model slr \
        -model-features f1:float:1 \
        -model-predict ans:int:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename dataset.csv

Assess the accuracy

.. code-block:: console
    :test:

    $ dffml accuracy \
        -model slr \
        -model-features f1:float:1 \
        -model-predict ans:int:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename dataset.csv
    1.0

Make a prediction

**predict.csv**

.. code-block::
    :test:
    :filepath: predict.csv

    f1
    0.8

.. code-block:: console
    :test:

    $ dffml predict all \
        -model slr \
        -model-features f1:float:1 \
        -model-predict ans:int:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename predict.csv
    [
        {
            "extra": {},
            "features": {
                "f1": 0.8
            },
            "key": "0",
            "last_updated": "2020-11-15T16:22:25Z",
            "prediction": {
                "ans": {
                    "confidence": 0.9355670103092784,
                    "value": 1
                }
            }
        }
    ]

Example usage of Logistic Regression using Python

**slr.py**

.. literalinclude:: ../../examples/model/slr/slr.py
    :test:

.. code-block:: console
    :test:

    $ python slr.py
    Accuracy: 0.9355670103092784
    {'f1': 0.8, 'ans': 1}

**Args**

- predict: Feature

  - Label or the value to be predicted

- features: List of features

  - Features to train on. For SLR only 1 allowed

- directory: Path

  - Directory where state should be saved

.. _plugin_model_dffml_model_tensorflow:

dffml_model_tensorflow
++++++++++++++++++++++

.. code-block:: console

    pip install dffml-model-tensorflow


.. note::

    It's important to keep the hidden layer config and feature config the same
    across invocations of train, predict, and accuracy methods.

    Models are saved under the ``directory`` parameter in subdirectories named
    after the hash of their feature names and hidden layer config. Which means
    if any of those parameters change between invocations, it's being told to
    look for a different saved model.

.. _plugin_model_dffml_model_tensorflow_tfdnnc:

tfdnnc
~~~~~~

*Official*

Implemented using Tensorflow's DNNClassifier.

First we create the training and testing datasets

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/train_data.sh

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/accuracy.sh

Output

.. code-block::

    0.99996233782

Make a prediction

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "PetalLength": 4.2,
                "PetalWidth": 1.5,
                "SepalLength": 5.9,
                "SepalWidth": 3.0,
                "classification": 1
            },
            "last_updated": "2019-07-31T02:00:12Z",
            "prediction": {
                "classification":
                    {
                        "confidence": 0.9999997615814209,
                        "value": 1
                    }
            },
            "key": "0"
        },
    ]

Example usage of Tensorflow DNNClassifier model using python API

.. literalinclude:: /../model/tensorflow/examples/tfdnnc/tfdnnc.py

**Args**

- predict: Feature

  - Feature name holding target values

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- steps: Integer

  - default: 3000
  - Number of steps to train the model

- epochs: Integer

  - default: 30
  - Number of iterations to pass over all records in a source

- hidden: List of integers

  - default: [12, 40, 15]
  - List length is the number of hidden layers in the network. Each entry in the list is the number of nodes in that hidden layer

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- batchsize: Integer

  - default: 20
  - Number records to pass through in an epoch

- shuffle: String

  - default: True
  - Randomise order of records in a batch

.. _plugin_model_dffml_model_tensorflow_tfdnnr:

tfdnnr
~~~~~~

*Official*

Implemented using Tensorflow's DNNEstimator.

Usage:

* predict: Name of the feature we are trying to predict or using for training.

Generating train and test data

* This creates files `train.csv` and `test.csv`,
  make sure to take a BACKUP of files with same name in the directory
  from where this command is run as it overwrites any existing files.

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/train_data.sh

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/accuracy.sh

Output

.. code-block::

    0.9468210011

Make a prediction

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "Feature1": 0.21,
                "Feature2": 0.18,
                "TARGET": 0.84
            },
            "last_updated": "2019-10-24T15:26:41Z",
            "prediction": {
                "TARGET" : {
                    "confidence": null,
                    "value": 1.1983429193496704
                }
            },
            "key": 0
        }
    ]

Example usage of Tensorflow DNNEstimator model using python API

.. literalinclude:: /../model/tensorflow/examples/tfdnnr/tfdnnr.py

The ``null`` in ``confidence`` is the expected behaviour. (See TODO in
predict).

**Args**

- predict: Feature

  - Feature name holding target values

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- steps: Integer

  - default: 3000
  - Number of steps to train the model

- epochs: Integer

  - default: 30
  - Number of iterations to pass over all records in a source

- hidden: List of integers

  - default: [12, 40, 15]
  - List length is the number of hidden layers in the network. Each entry in the list is the number of nodes in that hidden layer

.. _plugin_model_dffml_model_tensorflow_hub:

dffml_model_tensorflow_hub
++++++++++++++++++++++++++

.. code-block:: console

    pip install dffml-model-tensorflow-hub


.. _plugin_model_dffml_model_tensorflow_hub_text_classifier:

text_classifier
~~~~~~~~~~~~~~~

*Official*

Implemented using Tensorflow hub pretrained models.


.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/train_data.sh

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/test_data.sh

Train the model

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/train.sh

Assess the accuracy

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/accuracy.sh

Output

.. code-block::

    0.5

Make a prediction

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "sentence": "I am not feeling good",
                "sentiment": 0
            },
            "key": "0",
            "last_updated": "2020-05-14T20:14:30Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.9999992847442627,
                    "value": 1
                }
            }
        },
        {
            "extra": {},
            "features": {
                "sentence": "Our trip was full of adventures",
                "sentiment": 1
            },
            "key": "1",
            "last_updated": "2020-05-14T20:14:30Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.9999088048934937,
                    "value": 1
                }
            }
        }
    ]



Example usage of Tensorflow_hub Text Classifier model using python API

.. literalinclude:: /../model/tensorflow_hub/examples/tfhub_text_classifier/textclassifier.py

**Args**

- predict: Feature

  - Feature name holding classification value

- classifications: List of strings

  - Options for value of classification

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- trainable: String

  - default: True
  - Tweak pretrained model by training again

- batch_size: Integer

  - default: 120
  - Batch size

- max_seq_length: Integer

  - default: 256
  - Length of sentence, used in preprocessing of input for bert embedding

- add_layers: String

  - default: False
  - Add layers on the top of pretrianed model/layer

- embedType: String

  - default: None
  - Type of pretrained embedding model, required to be set to `bert` to use bert pretrained embedding

- layers: List of strings

  - default: None
  - Extra layers to be added on top of pretrained model

- model_path: String

  - default: https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1
  - Pretrained model path/url

- optimizer: String

  - default: adam
  - Optimizer used by model

- metrics: String

  - default: accuracy
  - Metric used to evaluate model

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- epochs: Integer

  - default: 10
  - Number of iterations to pass over all records in a source

.. _plugin_model_dffml_model_transformers:

dffml_model_transformers
++++++++++++++++++++++++

.. code-block:: console

    pip install dffml-model-transformers


.. _plugin_model_dffml_model_transformers_hfclassifier:

hfclassifier
~~~~~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Tensorflow based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets

.. literalinclude:: /../model/transformers/examples/classification/train_data.sh

.. literalinclude:: /../model/transformers/examples/classification/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/classification/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/classification/accuracy.sh

Output

.. code-block::

    0.6666666666666666


Make a prediction

.. literalinclude:: /../model/transformers/examples/classification/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "sentence": "Cats are stupid",
                "sentiment": 0
            },
            "key": "0",
            "last_updated": "2020-06-12T08:31:45Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.153812974691391,
                    "value": 1
                }
            }
        },
        {
            "extra": {},
            "features": {
                "sentence": "My office work is awesome",
                "sentiment": 1
            },
            "key": "1",
            "last_updated": "2020-06-12T08:31:45Z",
            "prediction": {
                "sentiment": {
                    "confidence": 0.13032104074954987,
                    "value": 1
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

**Args**

- features: List of features

  - Feature to train on

- predict: Feature

  - Feature holding target values

- label_list: List of strings

  - List of target labels

- cache_dir: String

  - Directory to store the pre-trained models downloaded from s3

- model_name_or_path: String

  - Path to pretrained model or model identifier from huggingface.co/models

- directory: String

  - The output directory where the model predictions and checkpoints will be written.

- logging_dir: String

  - Tensorboard log dir.

- from_pt: String

  - default: False
  - Whether to load model from pytorch checkpoint or .bin file

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- max_seq_length: Integer

  - default: 128
  - The maximum total input sequence length after tokenization. Sequences longer than this will be truncated, sequences shorter will be padded.

- overwrite_cache: String

  - default: False
  - Overwrite the cached training and evaluation sets

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- use_fast: String

  - default: False
  - Set this flag to use fast tokenization.

- doc_stride: Integer

  - default: 128
  - When splitting up a long document into chunks, how much stride to take between chunks.

- optimizer_name: String

  - default: adam
  - Name of a Tensorflow optimizer among "adadelta, adagrad, adam, adamax, ftrl, nadam, rmsprop, sgd, adamw"

- loss_name: String

  - default: SparseCategoricalCrossentropy
  - Name of a Tensorflow loss. For the list see: https://www.tensorflow.org/api_docs/python/tf/keras/losses

- gpus: String

  - default: 0
  - List of gpu devices. If only one, switch to single gpu strategy, if None takes all availabel gpus

- no_cuda: String

  - default: False
  - Do not use CUDA even when it is available

- end_lr: float

  - default: 0
  - End learning rate for optimizer

- debug: String

  - default: False
  - Activate the trace to record computation graphs and profiling information

- overwrite_directory: String

  - default: False
  - Overwrite the content of the output directory.Use this to continue training if directory points to a checkpoint directory.

- evaluate_during_training: String

  - default: False
  - Run evaluation during training at each logging step.

- per_device_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/TPU core/CPU for training.

- per_device_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/TPU core/CPU for evaluation.

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward/update pass.

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam.

- weight_decay: float

  - default: 0.0
  - Weight decay if we apply some.

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer.

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- num_train_epochs: float

  - default: 1
  - Total number of training epochs to perform.

- max_steps: Integer

  - default: -1
  - If > 0: set total number of training steps to perform. Override num_train_epochs.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- logging_first_step: String

  - default: False
  - Log and eval the first global_step

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_steps: Integer

  - default: 500
  - Save checkpoint every X updates steps.

- save_total_limit: Integer

  - default: None
  - Limit the total amount of checkpoints.Deletes the older checkpoints in the directory. Default is unlimited checkpoints

- seed: Integer

  - default: 42
  - random seed for initialization

- fp16: String

  - default: False
  - Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].See details at https://nvidia.github.io/apex/amp.html

- local_rank: Integer

  - default: -1
  - For distributed training: local_rank

- dataloader_drop_last: String

  - default: False
  - Drop the last incomplete batch if the length of the dataset is not divisible by the batch size

- past_index: Integer

  - default: -1
  - Some models can make use of the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will use the corresponding output (usually index 2) as the past state and feed it to the model at the next training step under the keyword argument `mems`

.. _plugin_model_dffml_model_transformers_ner_tagger:

ner_tagger
~~~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Tensorflow based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets

.. literalinclude:: /../model/transformers/examples/ner/train_data.sh

.. literalinclude:: /../model/transformers/examples/ner/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/ner/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/ner/accuracy.sh

Output

.. code-block::

    0.888888888888889

Make a prediction

.. literalinclude:: /../model/transformers/examples/ner/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "SentenceId": 1,
                "Words": "DFFML models can do NER"
            },
            "key": "0",
            "last_updated": "2020-06-11T17:40:54Z",
            "prediction": {
                "Tag": {
                    "confidence": null,
                    "value": [
                        {
                            "DFFML": "B-MISC"
                        },
                        {
                            "models": "I-MISC"
                        },
                        {
                            "can": "O"
                        },
                        {
                            "do": "B-MISC"
                        },
                        {
                            "NER": "B-MISC"
                        }
                    ]
                }
            }
        },
        {
            "extra": {},
            "features": {
                "SentenceId": 2,
                "Words": "DFFML models can do regression"
            },
            "key": "1",
            "last_updated": "2020-06-11T17:40:57Z",
            "prediction": {
                "Tag": {
                    "confidence": null,
                    "value": [
                        {
                            "DFFML": "B-MISC"
                        },
                        {
                            "models": "I-MISC"
                        },
                        {
                            "can": "O"
                        },
                        {
                            "do": "B-MISC"
                        },
                        {
                            "regression": "I-MISC"
                        }
                    ]
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

Example usage of NER model using python API

.. literalinclude:: /../model/transformers/examples/ner/ner_model.py

**Args**

- sid: Feature

  - Unique Id to identify words of each sentence (Sentence ID)

- words: Feature

  - Tokens to train NER model

- predict: Feature

  - NER Tags (B-MISC, I-PER, O etc.) for tokens

- model_name_or_path: String

  - Path to pre-trained model or shortcut name listed on https://huggingface.co/models

- directory: String

  - The output directory where the model checkpoints will be written

- cache_dir: String

  - Directory to store the pre-trained models downloaded from s3

- from_pt: String

  - default: False
  - Whether to load model from pytorch checkpoint or .bin file

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- overwrite_directory: String

  - default: False
  - Overwrite the content of the output directory.Use this to continue training if directory points to a checkpoint directory.

- max_seq_length: Integer

  - default: 128
  - The maximum total input sentence length after tokenization.Sequences longer than this will be truncated, sequences shorter will be padded

- max_steps: Integer

  - default: 0
  - If greater than zero then sets total number of training steps to perform. Overrides `epochs`

- fp16: String

  - default: False
  - Whether to use 16-bit (mixed) precision instead of 32-bit

- use_fast: String

  - default: False
  - Set this flag to use fast tokenization.

- ner_tags: List of strings

  - default: ['O', 'B-MISC', 'I-MISC', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']
  - List of all distinct NER Tags

- do_lower_case: String

  - default: False
  - Set this flag if using uncased model.

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward pass.

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam

- weight_decay: float

  - default: 0.0
  - Weight decay

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- epochs: Integer

  - default: 1
  - Total number of training epochs to perform.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- save_steps: Integer

  - default: 10
  - Save checkpoint every X update steps.

- seed: Integer

  - default: 2020
  - Random seed for initialization

- gpus: String

  - default: 0
  - List of gpu devices. If only one, switch to single gpu strategy, if None takes all availabel gpus

- tpu: String

  - default: None
  - The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url

- num_tpu_cores: Integer

  - default: 8
  - Total number of TPU cores to use.

- per_device_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU/TPU for training

- per_device_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU/TPU for assessing accuracy

- no_cuda: String

  - default: False
  - Avoid using CUDA when available

- optimizer_name: String

  - default: adam
  - Name of a Tensorflow optimizer among "adadelta, adagrad, adam, adamax, ftrl, nadam, rmsprop, sgd, adamw"

- loss_name: String

  - default: SparseCategoricalCrossentropy
  - Name of a Tensorflow loss. For the list see: https://www.tensorflow.org/api_docs/python/tf/keras/losses

- overwrite_cache: String

  - default: False
  - Overwrite the cached training and evaluation sets

- logging_dir: String

  - default: ~/.cache/dffml/transformers/log
  - Tensorboard log dir.

- logging_first_step: String

  - default: False
  - Log and eval the first global_step

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_total_limit: Integer

  - default: None
  - Limit the total amount of checkpoints.Deletes the older checkpoints in the directory. Default is unlimited checkpoints

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].See details at https://nvidia.github.io/apex/amp.html

- local_rank: Integer

  - default: -1
  - For distributed training: local_rank

- end_lr: float

  - default: 0
  - End learning rate for optimizer

- debug: String

  - default: False
  - Activate the trace to record computation graphs and profiling information

- num_train_epochs: float

  - default: 1
  - Total number of training epochs to perform.

- evaluate_during_training: String

  - default: False
  - Run evaluation during training at each logging step.

- dataloader_drop_last: String

  - default: False
  - Drop the last incomplete batch if the length of the dataset is not divisible by the batch size

- past_index: Integer

  - default: -1
  - Some models can make use of the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will use the corresponding output (usually index 2) as the past state and feed it to the model at the next training step under the keyword argument `mems`

.. _plugin_model_dffml_model_transformers_qa_model:

qa_model
~~~~~~~~

*Official*

Implemented using `HuggingFace Transformers <https://huggingface.co/transformers/index.html>`_ Pytorch based Models.
Description about pretrianed models can be found `here <https://huggingface.co/transformers/pretrained_models.html>`_

First we create the training and testing datasets by taking few data points from `SQuAD1.1 dataset <https://rajpurkar.github.io/SQuAD-explorer/>`_ `LICENSE <https://creativecommons.org/licenses/by-sa/4.0/legalcode>`_

.. literalinclude:: /../model/transformers/examples/qa/train_data.sh

.. literalinclude:: /../model/transformers/examples/qa/test_data.sh

Train the model

.. literalinclude:: /../model/transformers/examples/qa/train.sh

Assess the accuracy

.. literalinclude:: /../model/transformers/examples/qa/accuracy.sh

Output

.. code-block::

    0.0

Make a prediction

.. literalinclude:: /../model/transformers/examples/qa/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "answer_text": null,
                "answers": [
                    {
                        "answer_start": 0,
                        "text": " "
                    }
                ],
                "context": "Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.",
                "is_impossible": false,
                "question": "What sits on top of the Main Building at Notre Dame?",
                "start_pos_char": null,
                "title": "University_of_Notre_Dame"
            },
            "key": "5733be284776f4190066117e",
            "last_updated": "2020-06-23T17:54:03Z",
            "prediction": {
                "Answer": {
                    "confidence": null,
                    "value": {
                        "5733be284776f4190066117e": "a copper statue of Christ"
                    }
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

In the above train, accuracy and predict commands, :ref:`plugin_source_dffml_op` source is used to
read and parse data from json file before feeding it to the model. The function used by opsource to parse json data
is:

.. literalinclude:: /../model/transformers/dffml_model_transformers/qa/utils.py

The location of the function is passed using: 

.. code-block:: console

        -source-opimp dffml_model_transformers.qa.utils:parser

And the arguments to `parser` are passed by:

.. code-block:: console

        -source-args train.json True

where `train.json` is the name of file containing training data and the bool `True`
is value of the flag `is_training`.

**Args**

- model_type: String

  - Model type in the list: distilbert, albert, bart, longformer, xlm-roberta, roberta, bert, xlnet, flaubert, mobilebert, xlm, electra, reformer

- model_name_or_path: String

  - Path to pretrained model or model identifier from huggingface.co/models

- directory: String

  - The output directory where the model checkpoints and predictions will be written.

- cache_dir: String

  - Where do you want to store the pre-trained models downloaded from s3

- log_dir: String

  - Directory used by SummaryWriter for logging

- tokenizer_name: String

  - default: None
  - Pretrained tokenizer name or path if not the same as model_name

- from_tf: String

  - default: False
  - Whether to load model from tensorflow checkpoint or .h5 file

- config_name: String

  - default: None
  - Pretrained config name or path if not the same as model_name

- null_score_diff_threshold: String

  - default: 0.0
  - If null_score - best_non_null is greater than the threshold predict null.

- max_seq_length: Integer

  - default: 384
  - The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded.

- doc_stride: Integer

  - default: 128
  - When splitting up a long document into chunks, how much stride to take between chunks.

- max_query_length: Integer

  - default: 64
  - The maximum number of tokens for the question. Questions longer than this will be truncated to this length

- do_lower_case: String

  - default: False
  - Set this flag while using uncased model

- per_gpu_train_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU for training

- per_gpu_eval_batch_size: Integer

  - default: 8
  - Batch size per GPU/CPU for evaluation

- learning_rate: float

  - default: 5e-05
  - The initial learning rate for Adam

- gradient_accumulation_steps: Integer

  - default: 1
  - Number of updates steps to accumulate before performing a backward/update pass

- weight_decay: float

  - default: 0.0
  - Weight decay if we apply some.

- adam_epsilon: float

  - default: 1e-08
  - Epsilon for Adam optimizer

- max_grad_norm: float

  - default: 1.0
  - Max gradient norm.

- num_train_epochs: float

  - default: 1.0
  - Total number of training epoches to perform

- max_steps: Integer

  - default: -1
  - If > 0: set total number of training steps to perform. Override num_train_epochs.

- warmup_steps: Integer

  - default: 0
  - Linear warmup over warmup_steps.

- n_best_size: Integer

  - default: 20
  - The total number of n-best predictions to generate

- max_answer_length: Integer

  - default: 30
  - The maximum length of an answer that can be generated. This is needed because the start and end predictions are not conditioned on one another.

- lang_id: Integer

  - default: 0
  - language id of input for language-specific xlm models (see tokenization_xlm.PRETRAINED_INIT_CONFIGURATION)

- logging_steps: Integer

  - default: 500
  - Log every X updates steps.

- save_steps: Integer

  - default: 500
  - Save checkpoint every X update steps

- no_cuda: String

  - default: False
  - Whether not to use CUDA when available

- overwrite_directory: String

  - default: False
  - Overwrite the content of the output directory

- seed: Integer

  - default: 2020
  - random seed for initialization

- local_rank: Integer

  - default: -1
  - local_rank for distributed training on gpus

- fp16: Integer

  - default: False
  - Whether to use 16-bit (mixed) precision (through NVIDIA apex) insted of 32-bit

- fp16_opt_level: String

  - default: O1
  - For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details at https://nvidia.github.io/apex/amp.html

- threads: Integer

  - default: 1
  - Multiple threads for converting example to features

.. _plugin_model_dffml_model_scratch:

dffml_model_scratch
+++++++++++++++++++

.. code-block:: console

    pip install dffml-model-scratch


.. _plugin_model_dffml_model_scratch_scratchlgrsag:

scratchlgrsag
~~~~~~~~~~~~~

*Official*

Logistic Regression using stochastic average gradient descent optimizer

The dataset used for training

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/dataset.sh

Train the model

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/train.sh

Assess the accuracy

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/accuracy.sh

Output

.. code-block:: console

    1.0

Make a prediction

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/predict.sh

Output

.. code-block:: console

    [
        {
            "extra": {},
            "features": {
                "ans": 0,
                "f1": 0.8
            },
            "last_updated": "2020-03-19T13:41:08Z",
            "prediction": {
                "ans": {
                    "confidence": 1.0,
                    "value": 1
                }
            },
            "key": "0"
        }
    ]

Example usage of Logistic Regression using Python

.. literalinclude:: /../model/scratch/examples/scratchlgrsag/scratchlgrsag.py

**Args**

- predict: Feature

  - Label or the value to be predicted

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

.. _plugin_model_dffml_model_scikit:

dffml_model_scikit
++++++++++++++++++

.. code-block:: console

    pip install dffml-model-scikit


Machine Learning models implemented with `scikit-learn <https://scikit-learn.org/stable/>`_.
Models are saved under the directory in subdirectories named after the hash of
their feature names.

**General Usage:**

Training:

.. code-block:: console

    $ dffml train \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -model-SCIKIT_PARAMETER_NAME SCIKIT_PARAMETER_VALUE \
        -sources f=TRAINING_DATA_SOURCE_TYPE \
        -source-filename TRAINING_DATA_FILE_NAME \
        -log debug

Testing and Accuracy:

.. code-block:: console

    $ dffml accuracy \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -sources f=TESTING_DATA_SOURCE_TYPE \
        -source-filename TESTING_DATA_FILE_NAME \
        -log debug

Predicting with trained model:

.. code-block:: console

    $ dffml predict all \
        -model SCIKIT_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -sources f=PREDICT_DATA_SOURCE_TYPE \
        -source-filename PREDICT_DATA_FILE_NAME \
        -log debug


**Models Available:**

+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Type           | Model                         | Entrypoint     | Parameters                                                                                                                                                                                    |
+================+===============================+================+===============================================================================================================================================================================================+
| Regression     | LinearRegression              | scikitlr       | `scikitlr <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression/>`_                                             |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ElasticNet                    | scikiteln      | `scikiteln <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet/>`_                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RandomForestRegressor         | scikitrfr      | `scikitrfr <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html>`_                                                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BayesianRidge                 | scikitbyr      | `scikitbyr <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html#sklearn.linear_model.BayesianRidge/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Lasso                         | scikitlas      | `scikitlas <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso/>`_                                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ARDRegression                 | scikitard      | `scikitard <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html#sklearn.linear_model.ARDRegression/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RANSACRegressor               | scikitrsc      | `scikitrsc <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor/>`_                                              |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | DecisionTreeRegressor         | scikitdtr      | `scikitdtr <https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianProcessRegressor      | scikitgpr      | `scikitgpr <https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor/>`_                    |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | OrthogonalMatchingPursuit     | scikitomp      | `scikitomp <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html#sklearn.linear_model.OrthogonalMatchingPursuit/>`_                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Lars                          | scikitlars     | `scikitlars <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html#sklearn.linear_model.Lars/>`_                                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Ridge                         | scikitridge    | `scikitridge <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge/>`_                                                                |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Classification | KNeighborsClassifier          | scikitknn      | `scikitknn <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier/>`_                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AdaBoostClassifier            | scikitadaboost | `scikitadaboost <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier/>`_                                           |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianProcessClassifier     | scikitgpc      | `scikitgpc <https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html#sklearn.gaussian_process.GaussianProcessClassifier/>`_                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | DecisionTreeClassifier        | scikitdtc      | `scikitdtc <https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier/>`_                                                |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | RandomForestClassifier        | scikitrfc      | `scikitrfc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier/>`_                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | QuadraticDiscriminantAnalysis | scikitqda      | `scikitqda <https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis/>`_|
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MLPClassifier                 | scikitmlp      | `scikitmlp <https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier/>`_                                              |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GaussianNB                    | scikitgnb      | `scikitgnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB/>`_                                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | SVC                           | scikitsvc      | `scikitsvc <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC/>`_                                                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | LogisticRegression            | scikitlor      | `scikitlor <https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression/>`_                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | GradientBoostingClassifier    | scikitgbc      | `scikitgbc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier/>`_                                |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BernoulliNB                   | scikitbnb      | `scikitbnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB/>`_                                                        |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | ExtraTreesClassifier          | scikitetc      | `scikitetc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier/>`_                                            |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | BaggingClassifier             | scikitbgc      | `scikitbgc <https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier/>`_                                                  |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | LinearDiscriminantAnalysis    | scikitlda      | `scikitlda <https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis/>`_      |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MultinomialNB                 | scikitmnb      | `scikitmnb <https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB/>`_                                                    |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Clustering     | KMeans                        | scikitkmeans   | `scikitkmeans <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans/>`_                                                                       |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | Birch                         | scikitbirch    | `scikitbirch <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch/>`_                                                                          |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MiniBatchKMeans               | scikitmbkmeans | `scikitmbkmeans <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans/>`_                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AffinityPropagation           | scikitap       | `scikitap <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation/>`_                                                 |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | MeanShift                     | scikitms       | `scikitms <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html#sklearn.cluster.MeanShift/>`_                                                                     |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | SpectralClustering            | scikitsc       | `scikitsc <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering/>`_                                                   |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | AgglomerativeClustering       | scikitac       | `scikitac <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering/>`_                                         |
|                +-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                | OPTICS                        | scikitoptics   | `scikitoptics <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html#sklearn.cluster.OPTICS/>`_                                                                       |
+----------------+-------------------------------+----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+


**Usage Example:**

Example below uses LinearRegression Model using the command line.

Let us take a simple example:

+----------------------+------------+--------------+--------+
| Years of Experience  |  Expertise | Trust Factor | Salary |
+======================+============+==============+========+
|          0           |     01     |      0.2     |   10   |
+----------------------+------------+--------------+--------+
|          1           |     03     |      0.4     |   20   |
+----------------------+------------+--------------+--------+
|          2           |     05     |      0.6     |   30   |
+----------------------+------------+--------------+--------+
|          3           |     07     |      0.8     |   40   |
+----------------------+------------+--------------+--------+
|          4           |     09     |      1.0     |   50   |
+----------------------+------------+--------------+--------+
|          5           |     11     |      1.2     |   60   |
+----------------------+------------+--------------+--------+

First we create the files

.. literalinclude:: /../model/scikit/examples/lr/train_data.sh

.. literalinclude:: /../model/scikit/examples/lr/test_data.sh

Train the model

.. literalinclude:: /../model/scikit/examples/lr/train.sh

Assess accuracy

.. literalinclude:: /../model/scikit/examples/lr/accuracy.sh

Output:

.. code-block::

    1.0

Make a prediction

.. literalinclude:: /../model/scikit/examples/lr/predict.sh

Output:

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "Expertise": 13,
                "Trust": 0.7,
                "Years": 6
            },
            "key": "0",
            "last_updated": "2020-03-01T22:26:46Z",
            "prediction": {
                "Salary": {
                    "confidence": 1.0,
                    "value": 70.0
                }
            }
        }
    ]


Example usage of Linear Regression Model using python API:

.. literalinclude:: /../model/scikit/examples/lr/lr.py

Example below uses KMeans Clustering Model on a small randomly generated dataset.

.. code-block:: console

    $ cat > train.csv << EOF
   Col1,          Col2,        Col3,         Col4
   5.05776417,   8.55128116,   6.15193196,  -8.67349666
   3.48864265,  -7.25952218,  -4.89216256,   4.69308946
   -8.16207603,  5.16792984,  -2.66971993,   0.2401882
   6.09809669,   8.36434181,   6.70940915,  -7.91491768
   -9.39122566,  5.39133807,  -2.29760281,  -1.69672981
   0.48311336,   8.19998973,   7.78641979,   7.8843821
   2.22409135,  -7.73598586,  -4.02660224,   2.82101794
   2.8137247 ,   8.36064298,   7.66196849,   3.12704676
   EOF
    $ cat > test.csv << EOF
   Col1,             Col2,          Col3,         Col4,    cluster
   -10.16770144,   2.73057215,  -1.49351481,   2.43005691,    6
   3.59705381,  -4.76520663,  -3.34916068,   5.72391486,     1
   4.01612313,  -4.641852  ,  -4.77333308,   5.87551683,     0
   EOF
    $ dffml train \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename train.csv \
        -source-readonly \
        -log debug
    $ dffml accuracy \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1\
        -model-tcluster cluster:int:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename test.csv \
        -source-readonly \
        -log debug
    0.6365141682948129
    $ echo -e 'Col1,Col2,Col3,Col4\n6.09809669,8.36434181,6.70940915,-7.91491768\n' | \
      dffml predict all \
        -model scikitkmeans \
        -model-features Col1:float:1 Col2:float:1 Col3:float:1 Col4:float:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename /dev/stdin \
        -source-readonly \
        -log debug
    [
        {
            "extra": {},
            "features": {
                "Col1": 6.09809669,
                "Col2": 8.36434181,
                "Col3": 6.70940915,
                "Col4": -7.91491768
            },
            "last_updated": "2020-01-12T22:51:15Z",
            "prediction": {
                "confidence": 0.6365141682948129,
                "value": 2
            },
            "key": "0"
        }
    ]

Example usage of KMeans Clustering Model using python API:

.. code-block:: python

    from dffml import CSVSource, Features, Feature
    from dffml.noasync import train, accuracy, predict
    from dffml_model_scikit import KMeansModel

    model = KMeansModel(
        features=Features(
            Feature("Col1", float, 1),
            Feature("Col2", float, 1),
            Feature("Col3", float, 1),
            Feature("Col4", float, 1),
        ),
        tcluster=Feature("cluster", int, 1),
        directory="tempdir",
    )

    # Train the model
    train(model, "train.csv")

    # Assess accuracy (alternate way of specifying data source)
    print("Accuracy:", accuracy(model, CSVSource(filename="test.csv")))

    # Make prediction
    for i, features, prediction in predict(
        model,
        {"Col1": 6.09809669, "Col2": 8.36434181, "Col3": 6.70940915, "Col4": -7.91491768},
    ):
        features["cluster"] = prediction["cluster"]["value"]
        print(features)

**NOTE**: `Transductive <https://scikit-learn.org/stable/glossary.html#term-transductive/>`_ Clusterers(scikitsc, scikitac, scikitoptics) cannot handle unseen data.
Ensure that `predict` and `accuracy` for these algorithms uses training data.

**Args**

- predict: Feature

  - Label or the value to be predicted
  - Only used by classification and regression models

- tcluster: Feature

  - True cluster, only used by clustering models
  - Passed with `accuracy` to return `mutual_info_score`
  - If not passed `accuracy` returns `silhouette_score`

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved



.. _plugin_model_dffml_model_vowpalWabbit:

dffml_model_vowpalWabbit
++++++++++++++++++++++++

.. code-block:: console

    pip install dffml-model-vowpalWabbit


.. _plugin_model_dffml_model_vowpalWabbit_vwmodel:

vwmodel
~~~~~~~

*Official*

Implemented using Vowpal Wabbit.

First we create the training and testing datasets

.. literalinclude:: /../model/vowpalWabbit/examples/train_data.sh

.. literalinclude:: /../model/vowpalWabbit/examples/test_data.sh

Train the model

.. literalinclude:: /../model/vowpalWabbit/examples/train.sh

Assess the accuracy

.. literalinclude:: /../model/vowpalWabbit/examples/accuracy.sh

Output

.. code-block::

    0.38683876649129145


Make a prediction

.. literalinclude:: /../model/vowpalWabbit/examples/predict.sh

Output

.. code-block:: json

    [
        {
            "extra": {},
            "features": {
                "A": "| price:.46 sqft:.4 age:.10 1924"
            },
            "key": "0",
            "last_updated": "2020-05-29T16:36:57Z",
            "prediction": {
                "B": {
                    "confidence": 0.38683876649129145,
                    "value": 0.0
                }
            }
        }
    ]

**Args**

- features: List of features

- predict: Feature

  - Feature to predict

- directory: Path

  - Directory where state should be saved

- class_cost: List of features

  - default: None
  - Features with name `Cost_{class}` contaning cost of `class` for each input example, used when `csoaa` is used

- task: String

  - default: regression
  - Task to perform, possible values are `classification`, `regression`

- use_binary_label: String

  - default: False
  - Convert target labels to -1 and 1 for binary classification

- vwcmd: List of strings

  - default: []
  - Command Line Arguements as per vowpal wabbit convention

- namespace: List of strings

  - default: []
  - Namespace for input features. Should be in format {namespace}_{feature name}

- importance: Feature

  - default: None
  - Feature containing `importance` of each example, used in conversion of input data to vowpal wabbit input format

- base: Feature

  - default: None
  - Feature containing `base` for each example, used for residual regression

- tag: Feature

  - default: None
  - Feature to be used as `tag` in conversion of data to vowpal wabbit input format

- noconvert: String

  - default: False
  - Do not convert record features to vowpal wabbit input format

.. _plugin_model_dffml_model_spacy:

dffml_model_spacy
+++++++++++++++++

.. code-block:: console

    pip install dffml-model-spacy


.. _plugin_model_dffml_model_spacy_spacyner:

spacyner
~~~~~~~~

*Official*

Implemented using `Spacy statistical models <https://spacy.io/usage/training>`_ .

.. note::

    You must download ``en_core_web_sm`` before using this model

    .. code-block:: console
        :test:

        $ python -m spacy download en_core_web_sm

First we create the training and testing datasets.

Training data:

**train.json**

.. code-block:: json
    :test:
    :filepath: train.json

    {
        "data": [
            {
                "sentence": "I went to London and Berlin.",
                "entities": [
                    {
                        "start":10,
                        "end": 16,
                        "tag": "LOC"
                    },
                    {
                        "start":21,
                        "end": 27,
                        "tag": "LOC"
                    }
                ]
            },
            {
                "sentence": "Who is Alex?",
                "entities": [
                    {
                        "start":7,
                        "end": 11,
                        "tag": "PERSON"
                    }
                ]
            }
        ]
    }

Testing data:

**test.json**

.. code-block:: json
    :test:
    :filepath: test.json

    {
        "data": [
            {
                "sentence": "Alex went to London?"
            }
        ]
    }

Train the model

.. code-block:: console
    :test:

    $ dffml train \
        -model spacyner \
        -sources s=op \
        -source-opimp dffml_model_spacy.ner.utils:parser \
        -source-args train.json False \
        -model-model_name_or_path en_core_web_sm \
        -model-directory temp \
        -model-n_iter 5 \
        -log debug

Assess the accuracy

.. code-block:: console
    :test:

    $ dffml accuracy \
        -model spacyner \
        -sources s=op \
        -source-opimp dffml_model_spacy.ner.utils:parser \
        -source-args train.json False \
        -model-model_name_or_path en_core_web_sm \
        -model-directory temp \
        -model-n_iter 5 \
        -log debug
    0.0

Make a prediction

.. code-block:: console
    :test:

    $ dffml predict all \
        -model spacyner \
        -sources s=op \
        -source-opimp dffml_model_spacy.ner.utils:parser \
        -source-args test.json True \
        -model-model_name_or_path en_core_web_sm \
        -model-directory temp \
        -model-n_iter 5 \
        -log debug
    [
        {
            "extra": {},
            "features": {
                "entities": [],
                "sentence": "Alex went to London?"
            },
            "key": 0,
            "last_updated": "2020-07-27T16:26:18Z",
            "prediction": {
                "Answer": {
                    "confidence": null,
                    "value": [
                        [
                            "Alex",
                            "PERSON"
                        ],
                        [
                            "London",
                            "GPE"
                        ]
                    ]
                }
            }
        }
    ]

The model can be trained on large datasets to get the expected
output. The example shown above is to demonstrate the commandline usage
of the model.

In the above train, accuracy and predict commands, :ref:`plugin_source_dffml_op` source is used to
read and parse data from json file before feeding it to the model. The function used by opsource to parse json data
is:

.. literalinclude:: /../model/spacy/dffml_model_spacy/ner/utils.py

The location of the function is passed using:

.. code-block:: console

        -source-opimp dffml_model_spacy.ner.utils:parser

And the arguments to `parser` are passed by:

.. code-block:: console

        -source-args train.json False

where `train.json` is the name of file containing training data and the bool `False`
is value of the flag `is_predicting`.

**Args**

- directory: String

  - Output directory

- model_name_or_path: String

  - default: None
  - Model name or path to saved model. Defaults to blank 'en' model.

- n_iter: Integer

  - default: 10
  - Number of training iterations

- dropout: float

  - default: 0.5
  - Dropout rate to be used during training

.. _plugin_model_dffml_model_pytorch:

dffml_model_pytorch
+++++++++++++++++++

.. code-block:: console

    pip install dffml-model-pytorch


Machine Learning models implemented with `PyTorch <https://pytorch.org/>`_.
Models are saved under the directory in `model.pt`.

**General Usage:**

Training:

.. code-block:: console

    $ dffml train \
        -model PYTORCH_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -model-CONFIGS CONFIG_VALUES \
        -sources f=TRAINING_DATA_SOURCE_TYPE \
        -source-CONFIGS TRAINING_DATA \
        -log debug

Testing and Accuracy:

.. code-block:: console

    $ dffml accuracy \
        -model PYTORCH_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -model-CONFIGS CONFIG_VALUES \
        -sources f=TESTING_DATA_SOURCE_TYPE \
        -source-CONFIGS TESTING_DATA \
        -log debug

Predicting with trained model:

.. code-block:: console

    $ dffml predict all \
        -model PYTORCH_MODEL_ENTRYPOINT \
        -model-features FEATURE_DEFINITION \
        -model-predict TO_PREDICT \
        -model-directory MODEL_DIRECTORY \
        -model-CONFIGS CONFIG_VALUES \
        -sources f=PREDICT_DATA_SOURCE_TYPE \
        -source-CONFIGS PREDICTION_DATA \
        -log debug


**Pre-Trained Models Available:**

+----------------+---------------------------------+--------------------+--------------------------------------------------------------------------------+
| Type           | Model                           | Entrypoint         | Architecture                                                                   |
+================+=================================+====================+================================================================================+
| Classification | AlexNet                         | alexnet            | `AlexNet architecture <https://arxiv.org/abs/1404.5997>`_                      |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | DenseNet-121                    | densenet121        | `DenseNet architecture <https://arxiv.org/pdf/1608.06993.pdf>`_                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | DenseNet-161                    | densenet161        |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | DenseNet-169                    | densenet169        |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | DenseNet-201                    | densenet201        |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | MnasNet 0.5                     | mnasnet0_5         | `MnasNet architecture <https://arxiv.org/pdf/1807.11626.pdf>`_                 |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | MnasNet 1.0                     | mnasnet1_0         |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | MobileNet V2                    | mobilenet_v2       | `MobileNet V2 architecture <https://arxiv.org/abs/1801.04381>`_                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-11                          | vgg11              | `VGG-11 architecture Configuration "A" <https://arxiv.org/pdf/1409.1556.pdf>`_ |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-11 with batch normalization | vgg11_bn           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-13                          | vgg13              | `VGG-13 architecture Configuration "B" <https://arxiv.org/pdf/1409.1556.pdf>`_ |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-13 with batch normalization | vgg13_bn           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-16                          | vgg16              | `VGG-16 architecture Configuration "D" <https://arxiv.org/pdf/1409.1556.pdf>`_ |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-16 with batch normalization | vgg16_bn           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-19                          | vgg19              | `VGG-19 architecture Configuration "E" <https://arxiv.org/pdf/1409.1556.pdf>`_ |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | VGG-19 with batch normalization | vgg19_bn           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | GoogleNet                       | googlenet          | `GoogleNet architecture <http://arxiv.org/abs/1409.4842>`_                     |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | Inception V3                    | inception_v3       | `Inception V3 architecture <http://arxiv.org/abs/1512.00567>`_                 |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNet-18                       | resnet18           | `ResNet architecture <https://arxiv.org/pdf/1512.03385.pdf>`_                  |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNet-34                       | resnet34           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNet-50                       | resnet50           |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNet-101                      | resnet101          |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNet-152                      | resnet152          |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | Wide ResNet-101-2               | wide_resnet101_2   | `Wide Resnet architecture <https://arxiv.org/pdf/1605.07146.pdf>`_             |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | Wide ResNet-50-2                | wide_resnet50_2    |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ShuffleNet V2 0.5               | shufflenet_v2_x0_5 | `Shuffle Net V2 architecture <https://arxiv.org/abs/1807.11164>`_              |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ShuffleNet V2 1.0               | shufflenet_v2_x1_0 |                                                                                |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNext-101-32x8D               | resnext101_32x8d   | `ResNext architecture <https://arxiv.org/pdf/1611.05431.pdf>`_                 |
|                +---------------------------------+--------------------+--------------------------------------------------------------------------------+
|                | ResNext-50-32x4D                | resnext50_32x4d    |                                                                                |
+----------------+---------------------------------+--------------------+--------------------------------------------------------------------------------+


**Usage Example:**

Example below uses ResNet-18 Model using the command line.

Let us take a simple example: **Classifying Ants and Bees Images**

First, we download the dataset and verify with ``sha384sum``

.. code-block::

    curl -LO https://download.pytorch.org/tutorial/hymenoptera_data.zip
    sha384sum -c - << EOF
    491db45cfcab02d99843fbdcf0574ecf99aa4f056d52c660a39248b5524f9e6e8f896d9faabd27ffcfc2eaca0cec6f39  /home/tron/Desktop/Development/hymenoptera_data.zip
    EOF
    hymenoptera_data.zip: OK

Unzip the file

.. code-block::

    unzip hymenoptera_data.zip

Train the model

.. literalinclude:: /../model/pytorch/examples/resnet18/train.sh

Assess accuracy

.. literalinclude:: /../model/pytorch/examples/resnet18/accuracy.sh

Output:

.. code-block::

    0.9215686274509803

Create a csv file with the names of the images to predict, whether they are ants or bees.

.. literalinclude:: /../model/pytorch/examples/resnet18/unknown_data.sh

Make the predictions

.. literalinclude:: /../model/pytorch/examples/resnet18/predict.sh

Output:

.. literalinclude:: /../model/pytorch/examples/resnet18/output.txt

.. _plugin_model_dffml_model_pytorch_alexnet:

alexnet
~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_densenet121:

densenet121
~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_densenet161:

densenet161
~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_densenet169:

densenet169
~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_densenet201:

densenet201
~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_googlenet:

googlenet
~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_inception_v3:

inception_v3
~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_mnasnet0_5:

mnasnet0_5
~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_mnasnet1_0:

mnasnet1_0
~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_mobilenet_v2:

mobilenet_v2
~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_pytorchnet:

pytorchnet
~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- network: Network

  - default: None
  - Model

.. _plugin_model_dffml_model_pytorch_resnet101:

resnet101
~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnet152:

resnet152
~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnet18:

resnet18
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnet34:

resnet34
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnet50:

resnet50
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnext101_32x8d:

resnext101_32x8d
~~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_resnext50_32x4d:

resnext50_32x4d
~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_shufflenet_v2_x0_5:

shufflenet_v2_x0_5
~~~~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_shufflenet_v2_x1_0:

shufflenet_v2_x1_0
~~~~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg11:

vgg11
~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg11_bn:

vgg11_bn
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg13:

vgg13
~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg13_bn:

vgg13_bn
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg16:

vgg16
~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg16_bn:

vgg16_bn
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg19:

vgg19
~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_vgg19_bn:

vgg19_bn
~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_wide_resnet101_2:

wide_resnet101_2
~~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_pytorch_wide_resnet50_2:

wide_resnet50_2
~~~~~~~~~~~~~~~

*Official*

No description

**Args**

- predict: Feature

  - Feature name holding classification value

- features: List of features

  - Features to train on

- directory: Path

  - Directory where state should be saved

- classifications: List of strings

  - default: None
  - Options for value of classification

- clstype: Type

  - default: <class 'str'>
  - Data type of classifications values

- imageSize: Integer

  - default: None
  - Common size for all images to resize and crop to

- enableGPU: String

  - default: False
  - Utilize GPUs for processing

- epochs: Integer

  - default: 20
  - Number of iterations to pass over all records in a source

- batch_size: Integer

  - default: 32
  - Batch size

- validation_split: float

  - default: 0.0
  - Split training data for Validation

- patience: Integer

  - default: 5
  - Early stops the training if validation loss doesn't improve after a given patience

- loss: PyTorchLoss

  - default: <class 'dffml.base.CrossEntropyLossFunction'>
  - Loss Functions available in PyTorch

- optimizer: String

  - default: SGD
  - Optimizer Algorithms available in PyTorch

- normalize_mean: List of floats

  - default: None
  - Mean values for normalizing Tensor image

- normalize_std: List of floats

  - default: None
  - Standard Deviation values for normalizing Tensor image

- pretrained: String

  - default: True
  - Load Pre-trained model weights

- trainable: String

  - default: False
  - Tweak pretrained model by training again

- add_layers: String

  - default: False
  - Add layers on top of pretrained model

- layers: dict

  - default: None
  - Extra layers to be added on top of pretrained model

.. _plugin_model_dffml_model_xgboost:

dffml_model_xgboost
+++++++++++++++++++

.. code-block:: console

    pip install dffml-model-xgboost


.. _plugin_model_dffml_model_xgboost_xgbclassifier:

xgbclassifier
~~~~~~~~~~~~~

*Official*

Model using xgboost to perform classification prediction via gradient boosted trees.
XGBoost is a leading software library for working with standard tabular data (the type of data you store in Pandas DataFrames,
as opposed to more exotic types of data like images and videos). With careful parameter tuning, you can train highly accurate models.

Examples
--------

Command line usage

First download the training and test files, change the headers to DFFML
format. The first row is an encoding of the classifications, we want CSV
headers for the column names.

.. code-block::
    :test:

    $ wget http://download.tensorflow.org/data/iris_training.csv
    $ wget http://download.tensorflow.org/data/iris_test.csv
    $ sed -i 's/.*setosa,versicolor,virginica/SepalLength,SepalWidth,PetalLength,PetalWidth,classification/g' iris_training.csv iris_test.csv


Run the train command

.. code-block:: console
    :test:

    $ dffml train \
        -sources train=csv \
        -source-filename iris_training.csv \
        -model xgbclassifier \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model \
        -model-max_depth 3 \
        -model-learning_rate 0.01 \
        -model-learning_rate 0.01 \
        -model-n_estimators 200 \
        -model-reg_lambda 1 \
        -model-reg_alpha 0 \
        -model-gamma 0 \
        -model-colsample_bytree 0 \
        -model-subsample 1  


Assess the accuracy 

.. code-block:: console
    :test:

    $ dffml accuracy \
        -sources train=csv \
        -source-filename iris_test.csv \
        -model xgbclassifier \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model 


Make predictions

.. code-block:: console
    :test:

    $ dffml predict all \
        -sources train=csv \
        -source-filename iris_test.csv \
        -model xgbclassifier \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model 
        

Python usage

.. literalinclude:: /../model/xgboost/examples/iris_classification.py

Output

.. code-block::

    Test accuracy: 0.933333333333333
    Training accuracy: 0.9703703703703703

**Args**

- directory: Path

  - Directory where model should be saved

- features: List of features

  - Features on which we train the model

- predict: Feature

  - Value to be predicted

- learning_rate: float

  - default: 0.3
  - Learning rate to train with

- n_estimators: Integer

  - default: 100
  - Number of gradient boosted trees. Equivalent to the number of boosting rounds

- max_depth: Integer

  - default: 6
  - Maximium tree depth for base learners

- objective: String

  - default: multi:softmax
  - Objective in training

- subsample: float

  - default: 1
  - Subsample ratio of the training instance

- gamma: float

  - default: 0
  - Minimium loss reduction required to make a furthre partition on a leaf node

- n_jobs: Integer

  - default: -1
  - Number of parallel threads used to run xgboost

- colsample_bytree: float

  - default: 1
  - Subsample ratio of columns when constructing each tree

- booster: String

  - default: gbtree
  - Specify which booster to use: gbtree, gblinear or dart

- min_child_weight: float

  - default: 1
  - Minimum sum of instance weight(hessian) needed in a child

- reg_lambda: float

  - default: 1
  - L2 regularization term on weights. Increasing this value will make model more conservative

- reg_alpha: float

  - default: 0
  - L1 regularization term on weights. Increasing this value will make model more conservative

.. _plugin_model_dffml_model_xgboost_xgbregressor:

xgbregressor
~~~~~~~~~~~~

*Official*

Model using xgboost to perform regression prediction via gradient boosted trees
XGBoost is a leading software library for working with standard tabular data (the type of data you store in Pandas DataFrames,
as opposed to more exotic types of data like images and videos). With careful parameter tuning, you can train highly accurate models.

Examples
--------

Command line usage

First download the training and test files, change the headers to DFFML
format.

.. code-block::
    :test:

    $ wget http://download.tensorflow.org/data/iris_training.csv
    $ wget http://download.tensorflow.org/data/iris_test.csv
    $ sed -i 's/.*setosa,versicolor,virginica/SepalLength,SepalWidth,PetalLength,PetalWidth,classification/g' iris_training.csv iris_test.csv


Run the train command

.. code-block:: console
    :test:

    $ dffml train \
        -sources train=csv \
        -source-filename iris_training.csv \
        -model xgbregressor \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model \
        -model-max_depth 3 \
        -model-learning_rate 0.01 \
        -model-n_estimators 200 \
        -model-reg_lambda 1 \
        -model-reg_alpha 0 \
        -model-gamma 0 \
        -model-colsample_bytree 0 \
        -model-subsample 1  


Assess the accuracy 

.. code-block:: console
    :test:

    $ dffml accuracy \
        -sources train=csv \
        -source-filename iris_test.csv \
        -model xgbregressor \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model 
    
Output

.. code-block::

    accuracy: 0.8841466984766406


Make predictions

.. code-block:: console
    :test:

    $ dffml predict all \
        -sources train=csv \
        -source-filename iris_test.csv \
        -model xgbregressor \
        -model-features \
          SepalLength:float:1 \
          SepalWidth:float:1 \
          PetalLength:float:1 \
          PetalWidth:float:1 \
        -model-predict classification \
        -model-directory model 

Python usage

**run.py**

.. literalinclude:: /../model/xgboost/examples/diabetesregression.py
    :test:
    :filepath: run.py

Output

.. code-block::
    :test:

    $ python run.py
    Test accuracy: 0.6669655406927468
    Training accuracy: 0.819782501866115

**Args**

- directory: Path

  - Directory where model should be saved

- features: List of features

  - Features on which we train the model

- predict: Feature

  - Value to be predicted

- learning_rate: float

  - default: 0.05
  - Learning rate to train with

- n_estimators: Integer

  - default: 1000
  - Number of gradient boosted trees. Equivalent to the number of boosting rounds

- max_depth: Integer

  - default: 6
  - Maximium tree depth for base learners

- subsample: float

  - default: 1
  - Subsample ratio of the training instance

- gamma: float

  - default: 0
  - Minimium loss reduction required to make a furthre partition on a leaf node

- n_jobs: Integer

  - default: -1
  - Number of parallel threads used to run xgboost

- colsample_bytree: float

  - default: 1
  - Subsample ratio of columns when constructing each tree

- booster: String

  - default: gbtree
  - Specify which booster to use: gbtree, gblinear or dart

- min_child_weight: float

  - default: 0
  - Minimum sum of instance weight(hessian) needed in a child

- reg_lambda: float

  - default: 1
  - L2 regularization term on weights. Increasing this value will make model more conservative

- reg_alpha: float

  - default: 0
  - L1 regularization term on weights. Increasing this value will make model more conservative

.. _plugin_model_dffml_model_autosklearn:

dffml_model_autosklearn
+++++++++++++++++++++++

.. code-block:: console

    pip install dffml-model-autosklearn


.. warning:

    Follow these instructions before running the above install
    command to ensure that ``auto-sklearn`` installs correctly

**Ubuntu Installation**

To provide a C++11 building environment and the lateste SWIG version on Ubuntu, run:

.. code-block:: console

    $ sudo apt-get install build-essential swig

Install other PyPi dependencies with

.. code-block:: console

    $ python3 -m pip install cython liac-arff psutil
    $ curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 python3 -m pip install

For more information about installation visit https://automl.github.io/auto-sklearn/master/installation.html#installation

.. _plugin_model_dffml_model_autosklearn_autoclassifier:

autoclassifier
~~~~~~~~~~~~~~

*Official*

No description

**Args**

- features: List of features

  - Features to train on

- predict: Feature

  - Label or the value to be predicted

- directory: Path

  - Directory where state should be saved

- time_left_for_this_task: Integer

  - default: 3600
  - Time limit in seconds for the search of appropriate models. By increasing this value, *auto-sklearn* has a higher chance of finding better models.

- per_run_time_limit: Integer

  - default: None
  - Time limit for a single call to the machine learning model. Model fitting will be terminated if the machine learning algorithm runs over the time limit. Set this value high enough so that typical machine learning algorithms can be fit on the training data.

- initial_configurations_via_metalearning: Integer

  - default: 25
  - Initialize the hyperparameter optimization algorithm with this many configurations which worked well on previously seen datasets. Disable if the hyperparameter optimization algorithm should start from scratch.

- ensemble_size: Integer

  - default: 50
  - Number of models added to the ensemble built by *Ensemble selection from libraries of models*. Models are drawn with replacement.

- ensemble_nbest: Integer

  - default: 50
  - Only consider the ``ensemble_nbest`` models when building an ensemble.

- max_models_on_disc: Integer

  - default: 50
  - Defines the maximum number of models that are kept in the disc. The additional number of models are permanently deleted. Due to the nature of this variable, it sets the upper limit on how many models can be used for an ensemble. It must be an integer greater or equal than 1. If set to None, all models are kept on the disc.

- ensemble_memory_limit: Integer

  - default: 1024
  - Memory limit in MB for the ensemble building process. `auto-sklearn` will reduce the number of considered models (``ensemble_nbest``) if the memory limit is reached. If ``None``, no memory limit is enforced.

- seed: Integer

  - default: 1
  - Used to seed SMAC. Will determine the output file names.

- ml_memory_limit: Integer

  - default: 3072
  - Memory limit in MB for the machine learning algorithm. `auto-sklearn` will stop fitting the machine learning algorithm if it tries to allocate more than `ml_memory_limit` MB. If None is provided, no memory limit is set. In case of multi-processing, `ml_memory_limit` will be per job.

- include_estimators: typing.Any

  - default: None
  - If None, all possible estimators are used. Otherwise specifies set of estimators to use.

- exclude_estimators: typing.Any

  - default: None
  - If None, all possible estimators are used. Otherwise specifies set of estimators not to use. Incompatible with include_estimators.

- include_preprocessors: typing.Any

  - default: None
  - If None all possible preprocessors are used. Otherwise specifies set of preprocessors to use.

- exclude_preprocessors: typing.Any

  - default: None
  - If None all possible preprocessors are used. Otherwise specifies set of preprocessors not to use. Incompatible with include_preprocessors.

- resampling_strategy: String

  - default: holdout
  - how to to handle overfitting, might need 'resampling_strategy_arguments'  fit where possible 'folds' in scikit-learn model_selection module in scikit-learn model_selection module in scikit-learn model_selection module

- resampling_strategy_arguments: dict

  - default: None
  - * ``train_size`` should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. * ``shuffle`` determines whether the data is shuffled prior to splitting it into train and validation.   required by chosen class as specified in scikit-learn documentation. If arguments are not provided, scikit-learn defaults are used. If no defaults are available, an exception is raised. Refer to the 'n_splits' argument as 'folds'.

- tmp_folder: String

  - default: None
  - folder to store configuration output and log files, if ``None`` automatically use ``/tmp/autosklearn_tmp_$pid_$random_number``

- output_folder: String

  - default: None
  - folder to store predictions for optional test set, if ``None`` automatically use ``/tmp/autosklearn_output_$pid_$random_number``

- delete_tmp_folder_after_terminate: String

  - default: True
  - remove tmp_folder, when finished. If tmp_folder is None tmp_dir will always be deleted

- delete_output_folder_after_terminate: String

  - default: True
  - remove output_folder, when finished. If output_folder is None output_dir will always be deleted

- n_jobs: Integer

  - default: None
  - The number of jobs to run in parallel for ``fit()``. ``-1`` means using all processors. By default, Auto-sklearn uses a single core for fitting the machine learning model and a single core for fitting an ensemble. Ensemble building is not affected by ``n_jobs`` but can be controlled by the number of models in the ensemble. In contrast to most scikit-learn models, ``n_jobs`` given in the constructor is not applied to the ``predict()`` method. If ``dask_client`` is None, a new dask client is created.

- dask_client: typing.Any

  - default: None
  - User-created dask client, can be used to start a dask cluster and then attach auto-sklearn to it.

- disable_evaluator_output: String

  - default: False
  - If True, disable model and prediction output. Cannot be used together with ensemble building. ``predict()`` cannot be used when setting this True. Can also be used as a list to pass more fine-grained information on what to save. Allowed elements in the  optimization/validation set, which would later on be used to build an ensemble.

- smac_scenario_args: dict

  - default: None
  - Additional arguments inserted into the scenario of SMAC. See the for a list of available arguments.

- get_smac_object_callback: typing.Any

  - default: None
  - Callback function to create an object of class The function must accept the arguments ``scenario_dict``, ``instances``, ``num_params``, ``runhistory``, ``seed`` and ``ta``. This is an advanced feature. Use only if you are familiar with

- logging_config: dict

  - default: None
  - dictionary object specifying the logger configuration. If None, the default logging.yaml file is used, which can be found in the directory ``util/logging.yaml`` relative to the installation.

- metadata_directory: String

  - default: None
  - path to the metadata directory. If None, the default directory (autosklearn.metalearning.files) is used.

- metric: typing.Any

  - default: None
  - Metrics`_. If None is provided, a default metric is selected depending on the task.

.. _plugin_model_dffml_model_autosklearn_autoregressor:

autoregressor
~~~~~~~~~~~~~

*Official*

``autoregressor`` / ``AutoSklearnRegressorModel`` will use ``auto-sklearn``
to train the a scikit model for you.

This is AutoML, it will tune hyperparameters for you.

Implemented using `AutoSklearnRegressor <https://automl.github.io/auto-sklearn/master/api.html#regression>`_.

First we create the training and testing datasets

**train.csv**

.. code-block::
    :test:
    :filepath: train.csv

    Feature1,Feature2,TARGET
    0.93,0.68,3.89
    0.24,0.42,1.75
    0.36,0.68,2.75
    0.53,0.31,2.00
    0.29,0.25,1.32
    0.29,0.52,2.14

**test.csv**

.. code-block::
    :test:
    :filepath: test.csv

    Feature1,Feature2,TARGET
    0.57,0.84,3.65
    0.95,0.19,2.46
    0.23,0.15,0.93

Train the model

.. code-block:: console
    :test:

    $ dffml train \
        -model autoregressor \
        -model-predict TARGET:float:1 \
        -model-clstype int \
        -sources f=csv \
        -source-filename train.csv \
        -model-features \
          Feature1:float:1 \
          Feature2:float:1 \
        -model-time_left_for_this_task 120 \
        -model-per_run_time_limit 30 \
        -model-ensemble_size 50 \
        -model-delete_tmp_folder_after_terminate False \
        -model-directory tempdir \
        -log debug

Assess the accuracy

.. code-block:: console
    :test:

    $ dffml accuracy \
        -model autoregressor \
        -model-predict TARGET:float:1 \
        -model-directory tempdir \
        -sources f=csv \
        -source-filename test.csv \
        -model-features \
          Feature1:float:1 \
          Feature2:float:1 \
        -log critical
    0.9961211434899032

Make a file containing the data to predict on

**predict.csv**

.. code-block::
    :test:
    :filepath: predict.csv

    Feature1,Feature2
    0.57,0.84

Make a prediction

.. code-block:: console
    :test:

    $ dffml predict all \
        -model autoregressor \
        -model-directory tempdir \
        -model-predict TARGET:float:1 \
        -sources iris=csv \
        -model-features \
          Feature1:float:1 \
          Feature2:float:1 \
        -source-filename predict.csv
    [
        {
            "extra": {},
            "features": {
                "Feature1": 0.57,
                "Feature2": 0.84
            },
            "key": "0",
            "last_updated": "2020-11-23T05:52:13Z",
            "prediction": {
                "TARGET": {
                    "confidence": NaN,
                    "value": 3.566799074411392
                }
            }
        }
    ]

The model can be trained on large datasets to get better accuracy
output. The example shown above is to demonstrate the command line usage
of the model.

Example usage of using the model from Python

**run.py**

.. literalinclude:: /../model/autosklearn/examples/autoregressor.py
    :test:
    :filepath: run.py

Run the file

.. code-block:: console
    :test:

    $ python run.py
    Accuracy: 0.9961211434899032
    {'Feature1': 0.57, 'Feature2': 0.84, 'TARGET': 3.6180416345596313}

**Args**

- features: List of features

  - Features to train on

- predict: Feature

  - Label or the value to be predicted

- directory: Path

  - Directory where state should be saved

- time_left_for_this_task: Integer

  - default: 3600
  - Time limit in seconds for the search of appropriate models. By increasing this value, *auto-sklearn* has a higher chance of finding better models.

- per_run_time_limit: Integer

  - default: None
  - Time limit for a single call to the machine learning model. Model fitting will be terminated if the machine learning algorithm runs over the time limit. Set this value high enough so that typical machine learning algorithms can be fit on the training data.

- initial_configurations_via_metalearning: Integer

  - default: 25
  - Initialize the hyperparameter optimization algorithm with this many configurations which worked well on previously seen datasets. Disable if the hyperparameter optimization algorithm should start from scratch.

- ensemble_size: Integer

  - default: 50
  - Number of models added to the ensemble built by *Ensemble selection from libraries of models*. Models are drawn with replacement.

- ensemble_nbest: Integer

  - default: 50
  - Only consider the ``ensemble_nbest`` models when building an ensemble.

- max_models_on_disc: Integer

  - default: 50
  - Defines the maximum number of models that are kept in the disc. The additional number of models are permanently deleted. Due to the nature of this variable, it sets the upper limit on how many models can be used for an ensemble. It must be an integer greater or equal than 1. If set to None, all models are kept on the disc.

- ensemble_memory_limit: Integer

  - default: 1024
  - Memory limit in MB for the ensemble building process. `auto-sklearn` will reduce the number of considered models (``ensemble_nbest``) if the memory limit is reached. If ``None``, no memory limit is enforced.

- seed: Integer

  - default: 1
  - Used to seed SMAC. Will determine the output file names.

- ml_memory_limit: Integer

  - default: 3072
  - Memory limit in MB for the machine learning algorithm. `auto-sklearn` will stop fitting the machine learning algorithm if it tries to allocate more than `ml_memory_limit` MB. If None is provided, no memory limit is set. In case of multi-processing, `ml_memory_limit` will be per job.

- include_estimators: typing.Any

  - default: None
  - If None, all possible estimators are used. Otherwise specifies set of estimators to use.

- exclude_estimators: typing.Any

  - default: None
  - If None, all possible estimators are used. Otherwise specifies set of estimators not to use. Incompatible with include_estimators.

- include_preprocessors: typing.Any

  - default: None
  - If None all possible preprocessors are used. Otherwise specifies set of preprocessors to use.

- exclude_preprocessors: typing.Any

  - default: None
  - If None all possible preprocessors are used. Otherwise specifies set of preprocessors not to use. Incompatible with include_preprocessors.

- resampling_strategy: String

  - default: holdout
  - how to to handle overfitting, might need 'resampling_strategy_arguments'  fit where possible 'folds' in scikit-learn model_selection module in scikit-learn model_selection module in scikit-learn model_selection module

- resampling_strategy_arguments: dict

  - default: None
  - * ``train_size`` should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. * ``shuffle`` determines whether the data is shuffled prior to splitting it into train and validation.   required by chosen class as specified in scikit-learn documentation. If arguments are not provided, scikit-learn defaults are used. If no defaults are available, an exception is raised. Refer to the 'n_splits' argument as 'folds'.

- tmp_folder: String

  - default: None
  - folder to store configuration output and log files, if ``None`` automatically use ``/tmp/autosklearn_tmp_$pid_$random_number``

- output_folder: String

  - default: None
  - folder to store predictions for optional test set, if ``None`` automatically use ``/tmp/autosklearn_output_$pid_$random_number``

- delete_tmp_folder_after_terminate: String

  - default: True
  - remove tmp_folder, when finished. If tmp_folder is None tmp_dir will always be deleted

- delete_output_folder_after_terminate: String

  - default: True
  - remove output_folder, when finished. If output_folder is None output_dir will always be deleted

- n_jobs: Integer

  - default: None
  - The number of jobs to run in parallel for ``fit()``. ``-1`` means using all processors. By default, Auto-sklearn uses a single core for fitting the machine learning model and a single core for fitting an ensemble. Ensemble building is not affected by ``n_jobs`` but can be controlled by the number of models in the ensemble. In contrast to most scikit-learn models, ``n_jobs`` given in the constructor is not applied to the ``predict()`` method. If ``dask_client`` is None, a new dask client is created.

- dask_client: typing.Any

  - default: None
  - User-created dask client, can be used to start a dask cluster and then attach auto-sklearn to it.

- disable_evaluator_output: String

  - default: False
  - If True, disable model and prediction output. Cannot be used together with ensemble building. ``predict()`` cannot be used when setting this True. Can also be used as a list to pass more fine-grained information on what to save. Allowed elements in the  optimization/validation set, which would later on be used to build an ensemble.

- smac_scenario_args: dict

  - default: None
  - Additional arguments inserted into the scenario of SMAC. See the for a list of available arguments.

- get_smac_object_callback: typing.Any

  - default: None
  - Callback function to create an object of class The function must accept the arguments ``scenario_dict``, ``instances``, ``num_params``, ``runhistory``, ``seed`` and ``ta``. This is an advanced feature. Use only if you are familiar with

- logging_config: dict

  - default: None
  - dictionary object specifying the logger configuration. If None, the default logging.yaml file is used, which can be found in the directory ``util/logging.yaml`` relative to the installation.

- metadata_directory: String

  - default: None
  - path to the metadata directory. If None, the default directory (autosklearn.metalearning.files) is used.

- metric: typing.Any

  - default: None
  - Metrics`_. If None is provided, a default metric is selected depending on the task.