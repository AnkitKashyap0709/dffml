:github_url: hide

Operations
==========

Operations Implementations are subclasses of
:class:`dffml.df.base.OperationImplementation`, they are functions or classes
which could do anything, make HTTP requests, do inference, etc.

They don't necessarily have to be written in Python. Although DFFML isn't quite
to the point where it can use operations written in other languages yet, it's on
the roadmap.

.. _plugin_operation_dffml:

dffml
-----

.. code-block:: console

    pip install dffml


.. _plugin_operation_dffml_AcceptUserInput:

AcceptUserInput
~~~~~~~~~~~~~~~

*Official*

Accept input from stdin using python input()

Returns
+++++++
dict
    A dictionary containing user input.

Examples
++++++++

The following example shows how to use AcceptUserInput.
(Assumes that the input from stdio is "Data flow is awesome"!)

>>> import asyncio
>>> from dffml import *
>>>
>>> dataflow = DataFlow.auto(AcceptUserInput, GetSingle)
>>> dataflow.seed.append(
...     Input(
...         value=[AcceptUserInput.op.outputs["InputData"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, {"input": []}):
...         print(results)
>>>
>>> asyncio.run(main())
Enter the value: {'UserInput': 'Data flow is awesome'}

**Stage: processing**



**Outputs**

- InputData: UserInput(type: str)

.. _plugin_operation_dffml_associate:

associate
~~~~~~~~~

*Official*

No description

**Stage: output**



**Inputs**

- spec: associate_spec(type: List[str])

**Outputs**

- output: associate_output(type: Dict[str, Any])

.. _plugin_operation_dffml_associate_definition:

associate_definition
~~~~~~~~~~~~~~~~~~~~

*Official*

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> feed_def = Definition(name="feed", primitive="string")
>>> dead_def = Definition(name="dead", primitive="string")
>>> output = Definition(name="output", primitive="string")
>>>
>>> feed_input = Input(value="my favorite value", definition=feed_def)
>>> face_input = Input(
...     value="face", definition=output, parents=[feed_input]
... )
>>>
>>> dead_input = Input(
...     value="my second favorite value", definition=dead_def
... )
>>> beef_input = Input(
...     value="beef", definition=output, parents=[dead_input]
... )
>>>
>>> async def main():
...     for value in ["feed", "dead"]:
...         async for ctx, results in MemoryOrchestrator.run(
...             DataFlow.auto(AssociateDefinition),
...             [
...                 feed_input,
...                 face_input,
...                 dead_input,
...                 beef_input,
...                 Input(
...                     value={value: "output"},
...                     definition=AssociateDefinition.op.inputs["spec"],
...                 ),
...             ],
...         ):
...             print(results)
>>>
>>> asyncio.run(main())
{'feed': 'face'}
{'dead': 'beef'}

**Stage: output**



**Inputs**

- spec: associate_spec(type: List[str])

**Outputs**

- output: associate_output(type: Dict[str, Any])

.. _plugin_operation_dffml_db_query_create_table:

db_query_create_table
~~~~~~~~~~~~~~~~~~~~~

*Official*

Generates a create table query in the database.

Parameters
++++++++++
table_name : str
    The name of the table to be created.
cols : list[str]
    Columns of the table.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> dataflow = DataFlow(
...     operations={"db_query_create": db_query_create_table.op,},
...     configs={"db_query_create": DatabaseQueryConfig(database=sdb),},
...     seed=[],
... )
>>>
>>> inputs = [
...     Input(
...         value="myTable1",
...         definition=db_query_create_table.op.inputs["table_name"],
...     ),
...     Input(
...         value={
...             "key": "real",
...             "firstName": "text",
...             "lastName": "text",
...             "age": "real",
...         },
...         definition=db_query_create_table.op.inputs["cols"],
...     ),
... ]
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         pass
>>>
>>> asyncio.run(main())

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- cols: query_cols(type: Dict[str, str])

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_db_query_insert:

db_query_insert
~~~~~~~~~~~~~~~

*Official*

Generates an insert query in the database.

Parameters
++++++++++
table_name : str
    The name of the table to insert data in to.
data : dict
    Data to be inserted into the table.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> dataflow = DataFlow(
...     operations={
...         "db_query_insert": db_query_insert.op,
...         "db_query_lookup": db_query_lookup.op,
...         "get_single": GetSingle.imp.op,
...     },
...     configs={
...         "db_query_lookup": DatabaseQueryConfig(database=sdb),
...         "db_query_insert": DatabaseQueryConfig(database=sdb),
...     },
...     seed=[],
... )
>>>
>>> inputs = {
...     "insert": [
...         Input(
...             value="myTable", definition=db_query_insert.op.inputs["table_name"],
...         ),
...         Input(
...            value={"key": 10, "firstName": "John", "lastName": "Doe", "age": 16},
...             definition=db_query_insert.op.inputs["data"],
...         ),
...     ],
...     "lookup": [
...         Input(
...             value="myTable", definition=db_query_lookup.op.inputs["table_name"],
...         ),
...         Input(
...             value=["firstName", "lastName", "age"],
...             definition=db_query_lookup.op.inputs["cols"],
...         ),
...         Input(value=[], definition=db_query_lookup.op.inputs["conditions"],),
...         Input(
...             value=[db_query_lookup.op.outputs["lookups"].name],
...             definition=GetSingle.op.inputs["spec"],
...         ),
...     ]
... }
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         if result:
...             print(result)
>>>
>>> asyncio.run(main())
{'query_lookups': [{'firstName': 'John', 'lastName': 'Doe', 'age': 16}]}

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- data: query_data(type: Dict[str, Any])

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_db_query_insert_or_update:

db_query_insert_or_update
~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

Automatically uses the better suited operation, insert query or update query.

Parameters
++++++++++
table_name : str
    The name of the table to insert data in to.
data : dict
    Data to be inserted or updated into the table.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> person = {"key": 11, "firstName": "John", "lastName": "Wick", "age": 38}
>>>
>>> dataflow = DataFlow(
...     operations={
...         "db_query_insert_or_update": db_query_insert_or_update.op,
...         "db_query_lookup": db_query_lookup.op,
...         "get_single": GetSingle.imp.op,
...     },
...     configs={
...         "db_query_insert_or_update": DatabaseQueryConfig(database=sdb),
...         "db_query_lookup": DatabaseQueryConfig(database=sdb),
...     },
...     seed=[],
... )
>>>
>>> inputs = {
...     "insert_or_update": [
...         Input(
...             value="myTable", definition=db_query_update.op.inputs["table_name"],
...         ),
...         Input(
...             value=person,
...             definition=db_query_update.op.inputs["data"],
...         ),
...     ],
...     "lookup": [
...         Input(
...             value="myTable",
...             definition=db_query_lookup.op.inputs["table_name"],
...         ),
...         Input(
...             value=["firstName", "lastName", "age"],
...             definition=db_query_lookup.op.inputs["cols"],
...         ),
...         Input(value=[], definition=db_query_lookup.op.inputs["conditions"],),
...         Input(
...             value=[db_query_lookup.op.outputs["lookups"].name],
...             definition=GetSingle.op.inputs["spec"],
...         ),
...     ],
... }
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         if result:
...             print(result)
>>>
>>> asyncio.run(main())
{'query_lookups': [{'firstName': 'John', 'lastName': 'Wick', 'age': 38}]}
>>>
>>> person["age"] += 1
>>>
>>> asyncio.run(main())
{'query_lookups': [{'firstName': 'John', 'lastName': 'Wick', 'age': 39}]}

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- data: query_data(type: Dict[str, Any])

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_db_query_lookup:

db_query_lookup
~~~~~~~~~~~~~~~

*Official*

Generates a lookup query in the database.

Parameters
++++++++++
table_name : str
    The name of the table.
cols : list[str]
    Columns of the table.
conditions : Conditions
    Query conditions.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> dataflow = DataFlow(
...     operations={
...         "db_query_lookup": db_query_lookup.op,
...         "get_single": GetSingle.imp.op,
...     },
...     configs={"db_query_lookup": DatabaseQueryConfig(database=sdb),},
...     seed=[],
... )
>>>
>>> inputs = {
...     "lookup": [
...         Input(
...             value="myTable",
...             definition=db_query_lookup.op.inputs["table_name"],
...         ),
...         Input(
...             value=["firstName", "lastName", "age"],
...             definition=db_query_lookup.op.inputs["cols"],
...         ),
...         Input(value=[], definition=db_query_lookup.op.inputs["conditions"],),
...         Input(
...             value=[db_query_lookup.op.outputs["lookups"].name],
...             definition=GetSingle.op.inputs["spec"],
...         ),
...     ],
... }
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         if result:
...             print(result)
>>>
>>> asyncio.run(main())
{'query_lookups': [{'firstName': 'John', 'lastName': 'Doe', 'age': 16}, {'firstName': 'John', 'lastName': 'Wick', 'age': 39}]}

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- cols: query_cols(type: Dict[str, str])
- conditions: query_conditions(type: Conditions)

**Outputs**

- lookups: query_lookups(type: Dict[str, Any])

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_db_query_remove:

db_query_remove
~~~~~~~~~~~~~~~

*Official*

Generates a remove table query in the database.

Parameters
++++++++++
table_name : str
    The name of the table to insert data in to.
conditions : Conditions
    Query conditions.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> dataflow = DataFlow(
...     operations={
...         "db_query_lookup": db_query_lookup.op,
...         "db_query_remove": db_query_remove.op,
...         "get_single": GetSingle.imp.op,
...     },
...     configs={
...         "db_query_remove": DatabaseQueryConfig(database=sdb),
...         "db_query_lookup": DatabaseQueryConfig(database=sdb),
...     },
...     seed=[],
... )
>>>
>>> inputs = {
...     "remove": [
...         Input(
...             value="myTable",
...             definition=db_query_remove.op.inputs["table_name"],
...         ),
...         Input(value=[],
...         definition=db_query_remove.op.inputs["conditions"],),
...     ],
...     "lookup": [
...         Input(
...             value="myTable",
...             definition=db_query_lookup.op.inputs["table_name"],
...         ),
...         Input(
...             value=["firstName", "lastName", "age"],
...             definition=db_query_lookup.op.inputs["cols"],
...         ),
...         Input(value=[], definition=db_query_lookup.op.inputs["conditions"],),
...         Input(
...             value=[db_query_lookup.op.outputs["lookups"].name],
...             definition=GetSingle.op.inputs["spec"],
...         ),
...     ],
... }
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         if result:
...             print(result)
>>>
>>> asyncio.run(main())
{'query_lookups': []}

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- conditions: query_conditions(type: Conditions)

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_db_query_update:

db_query_update
~~~~~~~~~~~~~~~

*Official*

Generates an Update table query in the database.

Parameters
++++++++++
table_name : str
    The name of the table to insert data in to.
data : dict
    Data to be updated into the table.
conditions : list
    List of query conditions.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> sdb = SqliteDatabase(SqliteDatabaseConfig(filename="examples.db"))
>>>
>>> dataflow = DataFlow(
...     operations={
...         "db_query_update": db_query_update.op,
...         "db_query_lookup": db_query_lookup.op,
...         "get_single": GetSingle.imp.op,
...     },
...     configs={
...         "db_query_update": DatabaseQueryConfig(database=sdb),
...         "db_query_lookup": DatabaseQueryConfig(database=sdb),
...     },
...     seed=[],
... )
>>>
>>> inputs = {
...     "update": [
...         Input(
...             value="myTable",
...             definition=db_query_update.op.inputs["table_name"],
...         ),
...         Input(
...             value={
...                 "key": 10,
...                 "firstName": "John",
...                 "lastName": "Doe",
...                 "age": 17,
...             },
...             definition=db_query_update.op.inputs["data"],
...         ),
...         Input(value=[], definition=db_query_update.op.inputs["conditions"],),
...     ],
...     "lookup": [
...         Input(
...             value="myTable",
...             definition=db_query_lookup.op.inputs["table_name"],
...         ),
...         Input(
...             value=["firstName", "lastName", "age"],
...             definition=db_query_lookup.op.inputs["cols"],
...         ),
...         Input(value=[], definition=db_query_lookup.op.inputs["conditions"],),
...         Input(
...             value=[db_query_lookup.op.outputs["lookups"].name],
...             definition=GetSingle.op.inputs["spec"],
...         ),
...     ],
... }
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         if result:
...             print(result)
>>>
>>> asyncio.run(main())
{'query_lookups': [{'firstName': 'John', 'lastName': 'Doe', 'age': 17}]}

**Stage: processing**



**Inputs**

- table_name: query_table(type: str)
- data: query_data(type: Dict[str, Any])
- conditions: query_conditions(type: Conditions)

**Args**

- database: Entrypoint

.. _plugin_operation_dffml_dffml_dataflow_run:

dffml.dataflow.run
~~~~~~~~~~~~~~~~~~

*Official*

Starts a subflow ``self.config.dataflow`` and adds ``inputs`` in it.

Parameters
++++++++++
inputs : dict
    The inputs to add to the subflow. These should be a key value mapping of
    the context string to the inputs which should be seeded for that context
    string.

Returns
+++++++
dict
    Maps context strings in inputs to output after running through dataflow.

Examples
++++++++

The following shows how to use run dataflow in its default behavior.

>>> import asyncio
>>> from dffml import *
>>>
>>> URL = Definition(name="URL", primitive="string")
>>>
>>> subflow = DataFlow.auto(GetSingle)
>>> subflow.definitions[URL.name] = URL
>>> subflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> dataflow = DataFlow.auto(run_dataflow, GetSingle)
>>> dataflow.configs[run_dataflow.op.name] = RunDataFlowConfig(subflow)
>>> dataflow.seed.append(
...     Input(
...         value=[run_dataflow.op.outputs["results"].name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, {
...         "run_subflow": [
...             Input(
...                 value={
...                     "dffml": [
...                         {
...                             "value": "https://github.com/intel/dffml",
...                             "definition": URL.name
...                         }
...                     ]
...                 },
...                 definition=run_dataflow.op.inputs["inputs"]
...             )
...         ]
...     }):
...         print(results)
>>>
>>> asyncio.run(main())
{'flow_results': {'dffml': {'URL': 'https://github.com/intel/dffml'}}}

The following shows how to use run dataflow with custom inputs and outputs.
This allows you to run a subflow as if it were an operation.

>>> import asyncio
>>> from dffml import *
>>>
>>> URL = Definition(name="URL", primitive="string")
>>>
>>> @op(
...     inputs={"url": URL},
...     outputs={"last": Definition("last_element_in_path", primitive="string")},
... )
... def last_path(url):
...     return {"last": url.split("/")[-1]}
>>>
>>> subflow = DataFlow.auto(last_path, GetSingle)
>>> subflow.seed.append(
...     Input(
...         value=[last_path.op.outputs["last"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>>
>>> dataflow = DataFlow.auto(run_dataflow, GetSingle)
>>> dataflow.operations[run_dataflow.op.name] = run_dataflow.op._replace(
...     inputs={"URL": URL},
...     outputs={last_path.op.outputs["last"].name: last_path.op.outputs["last"]},
...     expand=[],
... )
>>> dataflow.configs[run_dataflow.op.name] = RunDataFlowConfig(subflow)
>>> dataflow.seed.append(
...     Input(
...         value=[last_path.op.outputs["last"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>> dataflow.update(auto_flow=True)
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(
...         dataflow,
...         {
...             "run_subflow": [
...                 Input(value="https://github.com/intel/dffml", definition=URL)
...             ]
...         },
...     ):
...         print(results)
>>>
>>> asyncio.run(main())
{'last_element_in_path': 'dffml'}

**Stage: processing**



**Inputs**

- inputs: flow_inputs(type: Dict[str,Any])

**Outputs**

- results: flow_results(type: Dict[str,Any])

**Args**

- dataflow: DataFlow

.. _plugin_operation_dffml_dffml_mapping_create:

dffml.mapping.create
~~~~~~~~~~~~~~~~~~~~

*Official*

Creates a mapping of a given key and value.

Parameters
++++++++++
key : str
    The key for the mapping.
value : Any
    The value for the mapping.

Returns
+++++++
dict
    A dictionary containing the mapping created.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> dataflow = DataFlow.auto(create_mapping, GetSingle)
>>> dataflow.seed.append(
...     Input(
...         value=[create_mapping.op.outputs["mapping"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>> inputs = [
...     Input(
...         value="key1", definition=create_mapping.op.inputs["key"],
...     ),
...     Input(
...         value=42, definition=create_mapping.op.inputs["value"],
...     ),
... ]
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         print(result)
>>>
>>> asyncio.run(main())
{'mapping': {'key1': 42}}

**Stage: processing**



**Inputs**

- key: key(type: str)
- value: value(type: generic)

**Outputs**

- mapping: mapping(type: map)

.. _plugin_operation_dffml_dffml_mapping_extract:

dffml.mapping.extract
~~~~~~~~~~~~~~~~~~~~~

*Official*

Extracts value from a given mapping.

Parameters
++++++++++
mapping : dict
    The mapping to extract the value from.
traverse : list[str]
    A list of keys to traverse through the mapping dictionary and extract the values.

Returns
+++++++
dict
    A dictionary containing the value of the keys.

Examples
++++++++

>>> import asyncio
>>> from dffml import *
>>>
>>> dataflow = DataFlow.auto(mapping_extract_value, GetSingle)
>>>
>>> dataflow.seed.append(
...     Input(
...         value=[mapping_extract_value.op.outputs["value"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>> inputs = [
...     Input(
...         value={"key1": {"key2": 42}},
...         definition=mapping_extract_value.op.inputs["mapping"],
...     ),
...     Input(
...         value=["key1", "key2"],
...         definition=mapping_extract_value.op.inputs["traverse"],
...     ),
... ]
>>>
>>> async def main():
...     async for ctx, result in MemoryOrchestrator.run(dataflow, inputs):
...         print(result)
>>>
>>> asyncio.run(main())
{'value': 42}

**Stage: processing**



**Inputs**

- mapping: mapping(type: map)
- traverse: mapping_traverse(type: List[str])

**Outputs**

- value: value(type: generic)

.. _plugin_operation_dffml_dffml_model_predict:

dffml.model.predict
~~~~~~~~~~~~~~~~~~~

*Official*

Predict using dffml models.

Parameters
++++++++++
features : dict
    A dictionary contaning feature name and feature value.

Returns
+++++++
dict
    A dictionary containing prediction.

Examples
++++++++

The following example shows how to use model_predict.

>>> import asyncio
>>> from dffml import *
>>>
>>> slr_model = SLRModel(
...     features=Features(Feature("Years", int, 1)),
...     predict=Feature("Salary", int, 1),
... )
>>> dataflow = DataFlow(
...     operations={
...         "prediction_using_model": model_predict,
...         "get_single": GetSingle,
...     },
...     configs={"prediction_using_model": ModelPredictConfig(model=slr_model)},
... )
>>> dataflow.seed.append(
...     Input(
...         value=[model_predict.op.outputs["prediction"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>>
>>> async def main():
...     await train(
...         slr_model,
...         {"Years": 0, "Salary": 10},
...         {"Years": 1, "Salary": 20},
...         {"Years": 2, "Salary": 30},
...         {"Years": 3, "Salary": 40},
...     )
...     inputs = [
...        Input(
...            value={"Years": 4}, definition=model_predict.op.inputs["features"],
...        )
...     ]
...     async for ctx, results in MemoryOrchestrator.run(dataflow, inputs):
...         print(results)
>>>
>>> asyncio.run(main())
{'model_predictions': {'Salary': {'confidence': 1.0, 'value': 50.0}}}

**Stage: processing**



**Inputs**

- features: record_features(type: Dict[str, Any])

**Outputs**

- prediction: model_predictions(type: Dict[str, Any])

**Args**

- model: Entrypoint

.. _plugin_operation_dffml_get_multi:

get_multi
~~~~~~~~~

*Official*

Output operation to get all Inputs matching given definitions.

Parameters
++++++++++
spec : list
    List of definition names. Any Inputs with matching definition will be
    returned.

Returns
+++++++
dict
    Maps definition names to all the Inputs of that definition

Examples
++++++++

The following shows how to grab all Inputs with the URL definition. If we
had we run an operation which output a URL, that output URL would have also
been returned to us.

>>> import asyncio
>>> from dffml import *
>>>
>>> URL = Definition(name="URL", primitive="string")
>>>
>>> dataflow = DataFlow.auto(GetMulti)
>>> dataflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetMulti.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, [
...         Input(
...             value="https://github.com/intel/dffml",
...             definition=URL
...         ),
...         Input(
...             value="https://github.com/intel/cve-bin-tool",
...             definition=URL
...         )
...     ]):
...         print(results)
...
>>> asyncio.run(main())
{'URL': ['https://github.com/intel/dffml', 'https://github.com/intel/cve-bin-tool']}

**Stage: output**



**Inputs**

- spec: get_n_spec(type: array)

**Outputs**

- output: get_n_output(type: map)

.. _plugin_operation_dffml_get_single:

get_single
~~~~~~~~~~

*Official*

Output operation to get a single Input for each definition given.

Parameters
++++++++++
spec : list
    List of definition names. An Input with matching definition will be
    returned.

Returns
+++++++
dict
    Maps definition names to an Input of that definition

Examples
++++++++

The following shows how to grab an Inputs with the URL definition. If we
had we run an operation which output a URL, that output URL could have also
been returned to us.

>>> import asyncio
>>> from dffml import *
>>>
>>> URL = Definition(name="URL", primitive="string")
>>>
>>> dataflow = DataFlow.auto(GetSingle)
>>> dataflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, [
...         Input(
...             value="https://github.com/intel/dffml",
...             definition=URL
...         )
...     ]):
...         print(results)
...
>>> asyncio.run(main())
{'URL': 'https://github.com/intel/dffml'}

**Stage: output**



**Inputs**

- spec: get_single_spec(type: array)

**Outputs**

- output: get_single_output(type: map)

.. _plugin_operation_dffml_group_by:

group_by
~~~~~~~~

*Official*

No description

**Stage: output**



**Inputs**

- spec: group_by_spec(type: Dict[str, Any])

**Outputs**

- output: group_by_output(type: Dict[str, List[Any]])

.. _plugin_operation_dffml_literal_eval:

literal_eval
~~~~~~~~~~~~

*Official*

Evaluate the input using ast.literal_eval()

Parameters
++++++++++
str_to_eval : str
    A string to be evaluated.

Returns
+++++++
dict
    A dict containing python literal.

Examples
++++++++

The following example shows how to use literal_eval.

>>> import asyncio
>>> from dffml import *
>>>
>>> dataflow = DataFlow.auto(literal_eval, GetSingle)
>>> dataflow.seed.append(
...    Input(
...        value=[literal_eval.op.outputs["str_after_eval"].name,],
...        definition=GetSingle.op.inputs["spec"],
...    )
... )
>>> inputs = [
...    Input(
...        value="[1,2,3]",
...        definition=literal_eval.op.inputs["str_to_eval"],
...        parents=None,
...    )
... ]
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, inputs):
...         print(results)
>>>
>>> asyncio.run(main())
{'EvaluatedStr': [1, 2, 3]}

**Stage: processing**



**Inputs**

- str_to_eval: InputStr(type: str)

**Outputs**

- str_after_eval: EvaluatedStr(type: generic)

.. _plugin_operation_dffml_print_output:

print_output
~~~~~~~~~~~~

*Official*

Print the output on stdout using python print()

Parameters
++++++++++
data : Any
    A python literal to be printed.

Examples
++++++++

The following example shows how to use print_output.

>>> import asyncio
>>> from dffml import *
>>>
>>> dataflow = DataFlow.auto(print_output)
>>> inputs = [
...     Input(
...         value="print_output example", definition=print_output.op.inputs["data"]
...     )
... ]
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, inputs):
...         pass
>>>
>>> asyncio.run(main())
print_output example

**Stage: processing**



**Inputs**

- data: DataToPrint(type: generic)

.. _plugin_operation_dffml_feature_git:

dffml_feature_git
-----------------

.. code-block:: console

    pip install dffml-feature-git


.. _plugin_operation_dffml_feature_git_check_if_valid_git_repository_URL:

check_if_valid_git_repository_URL
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- URL: URL(type: string)

**Outputs**

- valid: valid_git_repository_URL(type: boolean)

.. _plugin_operation_dffml_feature_git_cleanup_git_repo:

cleanup_git_repo
~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: cleanup**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str

.. _plugin_operation_dffml_feature_git_clone_git_repo:

clone_git_repo
~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- URL: URL(type: string)

**Outputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str

**Conditions**

- valid_git_repository_URL: boolean

.. _plugin_operation_dffml_feature_git_count_authors:

count_authors
~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- author_lines: author_line_count(type: Dict[str, int])

**Outputs**

- authors: author_count(type: int)

.. _plugin_operation_dffml_feature_git_git_commits:

git_commits
~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str
- branch: git_branch(type: str)
- start_end: date_pair(type: List[date])

**Outputs**

- commits: commit_count(type: int)

.. _plugin_operation_dffml_feature_git_git_repo_author_lines_for_dates:

git_repo_author_lines_for_dates
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str
- branch: git_branch(type: str)
- start_end: date_pair(type: List[date])

**Outputs**

- author_lines: author_line_count(type: Dict[str, int])

.. _plugin_operation_dffml_feature_git_git_repo_checkout:

git_repo_checkout
~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str
- commit: git_commit(type: string)

**Outputs**

- repo: git_repository_checked_out(type: Dict[str, str])

  - URL: str
  - directory: str
  - commit: str

.. _plugin_operation_dffml_feature_git_git_repo_commit_from_date:

git_repo_commit_from_date
~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str
- branch: git_branch(type: str)
- date: date(type: string)

**Outputs**

- commit: git_commit(type: string)

.. _plugin_operation_dffml_feature_git_git_repo_default_branch:

git_repo_default_branch
~~~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str

**Outputs**

- branch: git_branch(type: str)

**Conditions**

- no_git_branch_given: boolean

.. _plugin_operation_dffml_feature_git_git_repo_release:

git_repo_release
~~~~~~~~~~~~~~~~

*Official*

Was there a release within this date range

**Stage: processing**



**Inputs**

- repo: git_repository(type: Dict[str, str])

  - URL: str
  - directory: str
- branch: git_branch(type: str)
- start_end: date_pair(type: List[date])

**Outputs**

- present: release_within_period(type: bool)

.. _plugin_operation_dffml_feature_git_lines_of_code_by_language:

lines_of_code_by_language
~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

This operation relys on ``tokei``. Here's how to install version 10.1.1,
check it's releases page to make sure you're installing the latest version.

On Linux

.. code-block:: console

    $ curl -sSL 'https://github.com/XAMPPRocky/tokei/releases/download/v10.1.1/tokei-v10.1.1-x86_64-apple-darwin.tar.gz' \
      | tar -xvz && \
      echo '22699e16e71f07ff805805d26ee86ecb9b1052d7879350f7eb9ed87beb0e6b84fbb512963d01b75cec8e80532e4ea29a tokei' | sha384sum -c - && \
      sudo mv tokei /usr/local/bin/

On OSX

.. code-block:: console

    $ curl -sSL 'https://github.com/XAMPPRocky/tokei/releases/download/v10.1.1/tokei-v10.1.1-x86_64-apple-darwin.tar.gz' \
      | tar -xvz && \
      echo '8c8a1d8d8dd4d8bef93dabf5d2f6e27023777f8553393e269765d7ece85e68837cba4374a2615d83f071dfae22ba40e2 tokei' | sha384sum -c - && \
      sudo mv tokei /usr/local/bin/

**Stage: processing**



**Inputs**

- repo: git_repository_checked_out(type: Dict[str, str])

  - URL: str
  - directory: str
  - commit: str

**Outputs**

- lines_by_language: lines_by_language_count(type: Dict[str, Dict[str, int]])

.. _plugin_operation_dffml_feature_git_lines_of_code_to_comments:

lines_of_code_to_comments
~~~~~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- langs: lines_by_language_count(type: Dict[str, Dict[str, int]])

**Outputs**

- code_to_comment_ratio: language_to_comment_ratio(type: int)

.. _plugin_operation_dffml_feature_git_quarters_back_to_date:

quarters_back_to_date
~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- date: quarter_start_date(type: int)
- number: quarter(type: int)

**Outputs**

- date: date(type: string)
- start_end: date_pair(type: List[date])

.. _plugin_operation_dffml_feature_git_work:

work
~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- author_lines: author_line_count(type: Dict[str, int])

**Outputs**

- work: work_spread(type: int)

.. _plugin_operation_dffml_operations_binsec:

dffml_operations_binsec
-----------------------

.. code-block:: console

    pip install dffml-operations-binsec


.. _plugin_operation_dffml_operations_binsec_cleanup_rpm:

cleanup_rpm
~~~~~~~~~~~

*Official*

No description

**Stage: cleanup**



**Inputs**

- rpm: RPMObject(type: python_obj)

.. _plugin_operation_dffml_operations_binsec_files_in_rpm:

files_in_rpm
~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- rpm: RPMObject(type: python_obj)

**Outputs**

- files: rpm_filename(type: str)

.. _plugin_operation_dffml_operations_binsec_is_binary_pie:

is_binary_pie
~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- rpm: RPMObject(type: python_obj)
- filename: rpm_filename(type: str)

**Outputs**

- is_pie: binary_is_PIE(type: bool)

.. _plugin_operation_dffml_operations_binsec_url_to_urlbytes:

url_to_urlbytes
~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- URL: URL(type: string)

**Outputs**

- download: URLBytes(type: python_obj)

.. _plugin_operation_dffml_operations_binsec_urlbytes_to_rpmfile:

urlbytes_to_rpmfile
~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- download: URLBytes(type: python_obj)

**Outputs**

- rpm: RPMObject(type: python_obj)

.. _plugin_operation_dffml_operations_binsec_urlbytes_to_tarfile:

urlbytes_to_tarfile
~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- download: URLBytes(type: python_obj)

**Outputs**

- rpm: RPMObject(type: python_obj)

.. _plugin_operation_dffml_feature_auth:

dffml_feature_auth
------------------

.. code-block:: console

    pip install dffml-feature-auth


.. _plugin_operation_dffml_feature_auth_scrypt:

scrypt
~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- password: UnhashedPassword(type: string)

**Outputs**

- password: ScryptPassword(type: string)