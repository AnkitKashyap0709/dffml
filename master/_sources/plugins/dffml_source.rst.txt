:github_url: hide

.. _plugin_sources:

Sources
=======

Sources are implementations of :class:`dffml.source.source.BaseSource`, they
abstract the loading and storage of data / datasets.

If you want to get started creating your own source, check out the
:ref:`source_tutorial`.

.. _plugin_source_dffml:

dffml
+++++

.. code-block:: console

    pip install dffml


.. _plugin_source_dffml_csv:

csv
~~~

*Official*

Uses a CSV file as the source of record feature data

**Args**

- filename: String

- tag: String

  - default: untagged

- readwrite: String

  - default: False

- allowempty: String

  - default: False

- key: String

  - default: key

- tagcol: String

  - default: tag

- loadfiles: List of strings

  - default: None

.. _plugin_source_dffml_db:

db
~~

*Official*

No description

**Args**

- db: Entrypoint

- table_name: String

- model_columns: List of strings

.. _plugin_source_dffml_df:

df
~~

*Official*

>>> import asyncio
>>> from dffml import *
>>>
>>> records = [
...     Record(
...         "0",
...         data={
...             "features": {
...                 "Years": 1,
...                 "Expertise": 3,
...                 "Trust": 0.2,
...                 "Salary": 20,
...             }
...         },
...     ),
... ]
>>>
>>> features = Features(
...     Feature("Years", int, 1),
...     Feature("Expertise", int, 1),
...     Feature("Trust", float, 1),
...     Feature("Salary", int, 1),
... )
>>>
>>> dataflow = DataFlow(multiply, AssociateDefinition)
>>> dataflow.flow["multiply"].inputs["multiplicand"] = [
...     {"seed": ["Years", "Expertise", "Trust", "Salary"]}
... ]
>>> dataflow.seed = [
...     Input(
...         value={
...             feature.name: multiply.op.outputs["product"].name
...             for feature in features
...         },
...         definition=AssociateDefinition.op.inputs["spec"],
...     ),
...     Input(value=10, definition=multiply.op.inputs["multiplier"],),
... ]
>>>
>>>
>>> memory_source = Sources(MemorySource(MemorySourceConfig(records=records)))
>>>
>>> source = DataFlowSource(
...     DataFlowSourceConfig(
...         source=memory_source, dataflow=dataflow, features=features,
...     )
... )
>>>
>>>
>>> async def main():
...     async with source as src:
...         async with src() as sctx:
...             async for record in sctx.records():
...                 print(record.features())
...
>>>
>>> asyncio.run(main())
{'Years': 10, 'Expertise': 30, 'Trust': 2.0, 'Salary': 200}

**Args**

- source: Entrypoint

  - Source to wrap

- dataflow: DataFlow

  - DataFlow to use for preprocessing

- features: List of features

  - Features to pass as definitions to each context from each record to be preprocessed

- record_def: String

  - default: None
  - Definition to be used for record.key.If set, record.key will be added to the set of inputs under each context (which is also the record's key)

- length: String

  - default: None
  - Definition name to add as source length

- all_for_single: String

  - default: False
  - Run all records through dataflow before grabing results of desired record on a call to record()

- orchestrator: Entrypoint

  - default: MemoryOrchestrator(MemoryOrchestratorConfig(input_network=MemoryInputNetwork(MemoryInputNetworkConfig()), operation_network=MemoryOperationNetwork(MemoryOperationNetworkConfig(operations=[])), lock_network=MemoryLockNetwork(MemoryLockNetworkConfig()), opimp_network=MemoryOperationImplementationNetwork(MemoryOperationImplementationNetworkConfig(operations={})), rchecker=MemoryRedundancyChecker(MemoryRedundancyCheckerConfig(kvstore=MemoryKeyValueStore(MemoryKeyValueStoreConfig())))))

.. _plugin_source_dffml_dir:

dir
~~~

*Official*

Source to read files in a folder.

**Args**

- foldername: String

- feature: String

  - Name of the feature the data will be referenced as

- labels: List of strings

  - default: ['unlabelled']
  - Image labels

- save: Entrypoint

  - default: None

.. _plugin_source_dffml_idx1:

idx1
~~~~

*Official*

Source to read files in IDX1 format (such as MNIST digit label dataset).

**Args**

- filename: String

- feature: String

  - Name of the feature the data will be referenced as

- readwrite: String

  - default: False

- allowempty: String

  - default: False

.. _plugin_source_dffml_idx3:

idx3
~~~~

*Official*

Source to read files in IDX3 format (such as MNIST digit image dataset).

**Args**

- filename: String

- feature: String

  - Name of the feature the data will be referenced as

- readwrite: String

  - default: False

- allowempty: String

  - default: False

.. _plugin_source_dffml_ini:

ini
~~~

*Official*

Source to read files in .ini format.

**Args**

- filename: String

- readwrite: String

  - default: False

- allowempty: String

  - default: False

.. _plugin_source_dffml_json:

json
~~~~

*Official*

JSONSource reads and write from a JSON file on open / close. Otherwise
stored in memory.

**Args**

- filename: String

- tag: String

  - default: untagged

- readwrite: String

  - default: False

- allowempty: String

  - default: False

.. _plugin_source_dffml_memory:

memory
~~~~~~

*Official*

Stores records in a dict in memory

**Args**

- records: List of records

.. _plugin_source_dffml_op:

op
~~

*Official*

No description

**Args**

- opimp: OperationImplementation

- args: List of strings

  - default: []
  - Arguments to operation in input order

- allowempty: String

  - default: False
  - Raise an error if the source is empty after running the loading operation

.. _plugin_source_dffml_source_mysql:

dffml_source_mysql
++++++++++++++++++

.. code-block:: console

    pip install dffml-source-mysql


.. _plugin_source_dffml_source_mysql_mysql:

mysql
~~~~~

*Official*

No description

**Args**

- host: String

  - default: 127.0.0.1

- port: Integer

  - default: 3306

- user: String

- password: String

- db: String

- records-query: String

  - SELECT `key` as key, data_1 as feature_1, data_2 as feature_2 FROM record_data

- record-query: String

  - SELECT `key` as key, data_1 as feature_1, data_2 as feature_2 FROM record_data WHERE `key`=%s

- update-query: String

  - INSERT INTO record_data (`key`, data_1, data_2) VALUES(%s, %s, %s) ON DUPLICATE KEY UPDATE data_1 = %s, data_2=%s

- model-columns: List of strings

  - Order of Columns in table

- ca: String

  - default: None
  - Path to server TLS certificate