Operations
==========

Operations Implementations are subclasses of
:class:`dffml.df.base.OperationImplementation`, they are functions or classes
which could do anything, make HTTP requests, do inference, etc.

They don't necessarily have to be written in Python. Although DFFML isn't quite
to the point where it can use operations written in other languages yet, it's on
the roadmap.

.. _plugin_operation_dffml:

dffml
-----

.. code-block:: console

    pip install dffml


.. _plugin_operation_dffml_AcceptUserInput:

AcceptUserInput
~~~~~~~~~~~~~~~

*Official*

Accept input from stdin using python input()

Parameters
++++++++++
inputs : dict
    A dictionary with a key and empty list as value.

Returns
+++++++
dict
    A dictionary containing user input.

Examples
++++++++

The following example shows how to use AcceptUserInput.
(Assumes that the input from stdio is "Data flow is awesome"!)

>>> dataflow = DataFlow.auto(AcceptUserInput, GetSingle)
>>> dataflow.seed.append(
...     Input(
...         value=[AcceptUserInput.op.outputs["InputData"].name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, {"input":[]}):
...         print(results)
>>>
>>> asyncio.run(main())
Enter the value: {'UserInput': 'Data flow is awesome'}

**Stage: processing**



**Outputs**

- InputData: UserInput(type: str)

.. _plugin_operation_dffml_associate:

associate
~~~~~~~~~

*Official*

No description

**Stage: output**



**Inputs**

- spec: associate_spec(type: List[str])

**Outputs**

- output: associate_output(type: Dict[str, Any])

.. _plugin_operation_dffml_dffml_dataflow_run:

dffml.dataflow.run
~~~~~~~~~~~~~~~~~~

*Official*

Starts a subflow ``self.config.dataflow`` and adds ``inputs`` in it.

Parameters
++++++++++
inputs : dict
    The inputs to add to the subflow. These should be a key value mapping of
    the context string to the inputs which should be seeded for that context
    string.

Returns
+++++++
dict
    Maps context strings in inputs to output after running through dataflow.

Examples
++++++++

The following shows how to use run dataflow in its default behavior.

>>> URL = Definition(name="URL", primitive="string")
>>>
>>> subflow = DataFlow.auto(GetSingle)
>>> subflow.definitions[URL.name] = URL
>>> subflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> dataflow = DataFlow.auto(run_dataflow, GetSingle)
>>> dataflow.configs[run_dataflow.imp.op.name] = RunDataFlowConfig(subflow)
>>> dataflow.seed.append(
...     Input(
...         value=[run_dataflow.imp.op.outputs["results"].name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, {
...         "run_subflow": [
...             Input(
...                 value={
...                     "dffml": [
...                         {
...                             "value": "https://github.com/intel/dffml",
...                             "definition": URL.name
...                         }
...                     ]
...                 },
...                 definition=run_dataflow.imp.op.inputs["inputs"]
...             )
...         ]
...     }):
...         print(results)
>>>
>>> asyncio.run(main())
{'flow_results': {'dffml': {'URL': 'https://github.com/intel/dffml'}}}

The following shows how to use run dataflow with custom inputs and outputs.
This allows you to run a subflow as if it were an opertion.

>>> URL = Definition(name="URL", primitive="string")
>>>
>>> @op(
...     inputs={"url": URL},
...     outputs={"last": Definition("last_element_in_path", primitive="string")},
... )
... def last_path(url):
...     return {"last": url.split("/")[-1]}
>>>
>>> subflow = DataFlow.auto(last_path, GetSingle)
>>> subflow.seed.append(
...     Input(
...         value=[last_path.op.outputs["last"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>>
>>> dataflow = DataFlow.auto(run_dataflow, GetSingle)
>>> dataflow.operations[run_dataflow.op.name] = run_dataflow.op._replace(
...     inputs={"URL": URL},
...     outputs={last_path.op.outputs["last"].name: last_path.op.outputs["last"]},
...     expand=[],
... )
>>> dataflow.configs[run_dataflow.op.name] = RunDataFlowConfig(subflow)
>>> dataflow.seed.append(
...     Input(
...         value=[last_path.op.outputs["last"].name],
...         definition=GetSingle.op.inputs["spec"],
...     )
... )
>>> dataflow.update(auto_flow=True)
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(
...         dataflow,
...         {
...             "run_subflow": [
...                 Input(value="https://github.com/intel/dffml", definition=URL)
...             ]
...         },
...     ):
...         print(results)
>>>
>>> asyncio.run(main())
{'last_element_in_path': 'dffml'}

**Stage: processing**



**Inputs**

- inputs: flow_inputs(type: Dict[str,Any])

**Outputs**

- results: flow_results(type: Dict[str,Any])

**Args**

- dataflow: DataFlow

.. _plugin_operation_dffml_dffml_mapping_create:

dffml.mapping.create
~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- key: key(type: str)
- value: value(type: generic)

**Outputs**

- mapping: mapping(type: map)

.. _plugin_operation_dffml_dffml_mapping_extract:

dffml.mapping.extract
~~~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- mapping: mapping(type: map)
- traverse: mapping_traverse(type: List[str])

**Outputs**

- value: value(type: generic)

.. _plugin_operation_dffml_dffml_model_predict:

dffml.model.predict
~~~~~~~~~~~~~~~~~~~

*Official*

No description

**Stage: processing**



**Inputs**

- features: record_features(type: Dict[str, Any])

**Outputs**

- prediction: model_predictions(type: Dict[str, Any])

**Args**

- model: Entrypoint

.. _plugin_operation_dffml_get_multi:

get_multi
~~~~~~~~~

*Official*

Output operation to get all Inputs matching given definitions.

Parameters
++++++++++
spec : list
    List of definition names. Any Inputs with matching definition will be
    returned.

Returns
+++++++
dict
    Maps definition names to all the Inputs of that definition

Examples
++++++++

The following shows how to grab all Inputs with the URL definition. If we
had we run an operation which output a URL, that output URL would have also
been returned to us.

>>> URL = Definition(name="URL", primitive="string")
>>>
>>> dataflow = DataFlow.auto(GetMulti)
>>> dataflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetMulti.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, [
...         Input(
...             value="https://github.com/intel/dffml",
...             definition=URL
...         ),
...         Input(
...             value="https://github.com/intel/cve-bin-tool",
...             definition=URL
...         )
...     ]):
...         print(results)
...
>>> asyncio.run(main())
{'URL': ['https://github.com/intel/dffml', 'https://github.com/intel/cve-bin-tool']}

**Stage: output**



**Inputs**

- spec: get_n_spec(type: array)

**Outputs**

- output: get_n_output(type: map)

.. _plugin_operation_dffml_get_single:

get_single
~~~~~~~~~~

*Official*

Output operation to get a single Input for each definition given.

Parameters
++++++++++
spec : list
    List of definition names. An Input with matching definition will be
    returned.

Returns
+++++++
dict
    Maps definition names to an Input of that definition

Examples
++++++++

The following shows how to grab an Inputs with the URL definition. If we
had we run an operation which output a URL, that output URL could have also
been returned to us.

>>> URL = Definition(name="URL", primitive="string")
>>>
>>> dataflow = DataFlow.auto(GetSingle)
>>> dataflow.seed.append(
...     Input(
...         value=[URL.name],
...         definition=GetSingle.op.inputs["spec"]
...     )
... )
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, [
...         Input(
...             value="https://github.com/intel/dffml",
...             definition=URL
...         )
...     ]):
...         print(results)
...
>>> asyncio.run(main())
{'URL': 'https://github.com/intel/dffml'}

**Stage: output**



**Inputs**

- spec: get_single_spec(type: array)

**Outputs**

- output: get_single_output(type: map)

.. _plugin_operation_dffml_group_by:

group_by
~~~~~~~~

*Official*

No description

**Stage: output**



**Inputs**

- spec: group_by_spec(type: Dict[str, Any])

**Outputs**

- output: group_by_output(type: Dict[str, List[Any]])

.. _plugin_operation_dffml_literal_eval:

literal_eval
~~~~~~~~~~~~

*Official*

Evaluate the input using ast.literal_eval()

Parameters
++++++++++
inputs : str
    A string to be evaluated.

Returns
+++++++
A python literal.

Examples
++++++++

The following example shows how to use literal_eval.

>>> dataflow = DataFlow.auto(literal_eval, GetSingle)
>>> dataflow.seed.append(
...    Input(
...        value=[literal_eval.op.outputs["str_after_eval"].name,],
...        definition=GetSingle.op.inputs["spec"],
...    )
... )
>>> inputs = [
...    Input(
...        value="[1,2,3]",
...        definition=literal_eval.op.inputs["str_to_eval"],
...        parents=None,
...    )
... ]
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, inputs):
...         print(results)
>>>
>>> asyncio.run(main())
{'EvaluatedStr': [1, 2, 3]}

**Stage: processing**



**Inputs**

- str_to_eval: InputStr(type: str)

**Outputs**

- str_after_eval: EvaluatedStr(type: generic)

.. _plugin_operation_dffml_print_output:

print_output
~~~~~~~~~~~~

*Official*

Print the output on stdout using python print()

Parameters
++++++++++
inputs : list
    A list of Inputs whose value is to be printed.

Examples
++++++++

The following example shows how to use print_output.

>>> dataflow = DataFlow.auto(print_output, GetSingle)
>>> inputs = [
...     Input(
...         value="print_output example",
...         definition=dataflow.definitions["DataToPrint"],
...         parents=None,)]
>>>
>>> async def main():
...     async for ctx, results in MemoryOrchestrator.run(dataflow, inputs):
...         print("String to be printed is 'print_output example'")
>>>
>>> asyncio.run(main())
print_output example
String to be printed is 'print_output example'

**Stage: processing**



**Inputs**

- data: DataToPrint(type: str)